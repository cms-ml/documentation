<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Documentation of the CMS Machine Learning Group"><meta name=author content="CMS Machine Learning Group"><link rel=canonical href=https://cms-ml.github.io/documentation/optimization/model_optimization.html><link rel=prev href=../software_envs/containers.html><link rel=next href=importance.html><link rel=icon href=../images/favicon.png><meta name=generator content="mkdocs-1.4.2, mkdocs-material-9.0.3"><title>Model optimization - CMS Machine Learning Documentation</title><link rel=stylesheet href=../assets/stylesheets/main.6b71719e.min.css><link rel=stylesheet href=../assets/stylesheets/palette.2505c338.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=orange> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#model-optimization class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../index.html title="CMS Machine Learning Documentation" class="md-header__button md-logo" aria-label="CMS Machine Learning Documentation" data-md-component=logo> <img src=../images/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> CMS Machine Learning Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Model optimization </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=orange aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=orange aria-label="Switch to dark mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/cms-ml/documentation title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> cms-ml/documentation </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../index.html title="CMS Machine Learning Documentation" class="md-nav__button md-logo" aria-label="CMS Machine Learning Documentation" data-md-component=logo> <img src=../images/logo.png alt=logo> </a> CMS Machine Learning Documentation </label> <div class=md-nav__source> <a href=https://github.com/cms-ml/documentation title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> cms-ml/documentation </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../index.html class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_2 type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 tabindex=0 aria-expanded=false> Innovation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Innovation data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Innovation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../innovation/journal_club.html class=md-nav__link> ML Journal Club </a> </li> <li class=md-nav__item> <a href=../innovation/hackathons.html class=md-nav__link> ML Hackathons </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_3 type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 tabindex=0 aria-expanded=false> Resources <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Resources data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../resources/cloud_resources/index.html class=md-nav__link> Cloud Resources </a> </li> <li class=md-nav__item> <a href=../resources/dataset_resources/index.html class=md-nav__link> Dataset Resources </a> </li> <li class=md-nav__item> <a href=../resources/fpga_resources/index.html class=md-nav__link> FPGA Resource </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_3_4 type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4 tabindex=0 aria-expanded=false> GPU Resources <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="GPU Resources" data-md-level=2> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> GPU Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../resources/gpu_resources/cms_resources/lxplus_gpu.html class=md-nav__link> lxplus-gpu </a> </li> <li class=md-nav__item> <a href=../resources/gpu_resources/cms_resources/lxplus_htcondor.html class=md-nav__link> CERN HTCondor </a> </li> <li class=md-nav__item> <a href=../resources/gpu_resources/cms_resources/swan.html class=md-nav__link> SWAN </a> </li> <li class=md-nav__item> <a href=../resources/gpu_resources/cms_resources/ml_cern_ch.html class=md-nav__link> ml.cern.ch </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4 type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4 tabindex=0 aria-expanded=true> Guides <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Guides data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Guides </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_1 type=checkbox id=__nav_4_1> <label class=md-nav__link for=__nav_4_1 tabindex=0 aria-expanded=false> Software environments <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Software environments" data-md-level=2> <label class=md-nav__title for=__nav_4_1> <span class="md-nav__icon md-icon"></span> Software environments </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../software_envs/lcg_environments.html class=md-nav__link> LCG environments </a> </li> <li class=md-nav__item> <a href=../software_envs/containers.html class=md-nav__link> Using containers </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_2 type=checkbox id=__nav_4_2 checked> <label class=md-nav__link for=__nav_4_2 tabindex=0 aria-expanded=true> Optimization <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Optimization data-md-level=2> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Optimization </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Model optimization <span class="md-nav__icon md-icon"></span> </label> <a href=model_optimization.html class="md-nav__link md-nav__link--active"> Model optimization </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-we-talk-about-when-we-talk-about-model-optimization class=md-nav__link> What we talk about when we talk about model optimization </a> <nav class=md-nav aria-label="What we talk about when we talk about model optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#caveat-which-data-should-you-use-to-optimize-your-model class=md-nav__link> Caveat: which data should you use to optimize your model </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#grid-search class=md-nav__link> Grid Search </a> </li> <li class=md-nav__item> <a href=#random-search class=md-nav__link> Random search </a> </li> <li class=md-nav__item> <a href=#model-based-optimization-by-gradient-descent class=md-nav__link> Model-based optimization by gradient descent </a> <nav class=md-nav aria-label="Model-based optimization by gradient descent"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-based-optimization-by-surrogates class=md-nav__link> Model-based optimization by surrogates </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bayesian-optimization class=md-nav__link> Bayesian Optimization </a> <nav class=md-nav aria-label="Bayesian Optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gaussian-processes class=md-nav__link> Gaussian processes </a> </li> <li class=md-nav__item> <a href=#the-basic-idea-behind-bayesian-optimization class=md-nav__link> The basic idea behind Bayesian optimization </a> <nav class=md-nav aria-label="The basic idea behind Bayesian optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#historical-note class=md-nav__link> Historical note </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bayesian-optimization-in-practice class=md-nav__link> Bayesian optimization in practice </a> </li> <li class=md-nav__item> <a href=#limitations-and-some-workaround-of-bayesian-optimization class=md-nav__link> Limitations (and some workaround) of Bayesian Optimization </a> </li> <li class=md-nav__item> <a href=#alternatives-to-gaussian-processes-tree-based-models class=md-nav__link> Alternatives to Gaussian processes: Tree-based models </a> </li> <li class=md-nav__item> <a href=#implementations-of-bayesian-optimization class=md-nav__link> Implementations of Bayesian Optimization </a> </li> <li class=md-nav__item> <a href=#caveats-dont-get-too-obsessed-with-model-optimization class=md-nav__link> Caveats: don't get too obsessed with model optimization </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=importance.html class=md-nav__link> Feature importance </a> </li> <li class=md-nav__item> <a href=data_augmentation.html class=md-nav__link> Data augmentation </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_3 type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 tabindex=0 aria-expanded=false> General Advice <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="General Advice" data-md-level=2> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> General Advice </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../general_advice/intro.html class=md-nav__link> Introduction </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_3_2 type=checkbox id=__nav_4_3_2> <label class=md-nav__link for=__nav_4_3_2 tabindex=0 aria-expanded=false> Before training <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Before training" data-md-level=3> <label class=md-nav__title for=__nav_4_3_2> <span class="md-nav__icon md-icon"></span> Before training </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../general_advice/before/domains.html class=md-nav__link> Domains </a> </li> <li class=md-nav__item> <a href=../general_advice/before/features.html class=md-nav__link> Features </a> </li> <li class=md-nav__item> <a href=../general_advice/before/inputs.html class=md-nav__link> Inputs </a> </li> <li class=md-nav__item> <a href=../general_advice/before/model.html class=md-nav__link> Model </a> </li> <li class=md-nav__item> <a href=../general_advice/before/metrics.html class=md-nav__link> Metrics & Losses </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_3_3 type=checkbox id=__nav_4_3_3> <label class=md-nav__link for=__nav_4_3_3 tabindex=0 aria-expanded=false> During training <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="During training" data-md-level=3> <label class=md-nav__title for=__nav_4_3_3> <span class="md-nav__icon md-icon"></span> During training </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../general_advice/during/overfitting.html class=md-nav__link> Overfitting </a> </li> <li class=md-nav__item> <a href=../general_advice/during/xvalidation.html class=md-nav__link> Cross-validation </a> </li> <li class=md-nav__item> <a href=../general_advice/during/opt.html class=md-nav__link> Optimisation problems </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../general_advice/after/after.html class=md-nav__link> After training </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_4 type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 tabindex=0 aria-expanded=false> Inference <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Inference data-md-level=2> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> Inference </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_4_1 type=checkbox id=__nav_4_4_1> <label class=md-nav__link for=__nav_4_4_1 tabindex=0 aria-expanded=false> Direct inference <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Direct inference" data-md-level=3> <label class=md-nav__title for=__nav_4_4_1> <span class="md-nav__icon md-icon"></span> Direct inference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/tensorflow2.html class=md-nav__link> TensorFlow 2 </a> </li> <li class=md-nav__item> <a href=../inference/pytorch.html class=md-nav__link> PyTorch </a> </li> <li class=md-nav__item> <a href=../inference/pyg.html class=md-nav__link> PyTorch Geometric </a> </li> <li class=md-nav__item> <a href=../inference/onnx.html class=md-nav__link> ONNX </a> </li> <li class=md-nav__item> <a href=../inference/xgboost.html class=md-nav__link> XGBoost </a> </li> <li class=md-nav__item> <a href=../inference/hls4ml.html class=md-nav__link> hls4ml </a> </li> <li class=md-nav__item> <a href=../inference/conifer.html class=md-nav__link> conifer </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_4_2 type=checkbox id=__nav_4_4_2> <label class=md-nav__link for=__nav_4_4_2 tabindex=0 aria-expanded=false> Inference as a service <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Inference as a service" data-md-level=3> <label class=md-nav__title for=__nav_4_4_2> <span class="md-nav__icon md-icon"></span> Inference as a service </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/sonic_triton.html class=md-nav__link> Sonic/Triton </a> </li> <li class=md-nav__item> <a href=../inference/tfaas.html class=md-nav__link> TFaaS </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_4_3 type=checkbox id=__nav_4_4_3> <label class=md-nav__link for=__nav_4_4_3 tabindex=0 aria-expanded=false> Non-standard workflows <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Non-standard workflows" data-md-level=3> <label class=md-nav__title for=__nav_4_4_3> <span class="md-nav__icon md-icon"></span> Non-standard workflows </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/standalone.html class=md-nav__link> Standalone framework </a> </li> <li class=md-nav__item> <a href=../inference/swan_aws.html class=md-nav__link> SWAN + AWS </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../inference/checklist.html class=md-nav__link> Integration checklist </a> </li> <li class=md-nav__item> <a href=../inference/performance.html class=md-nav__link> Performance </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_4_6 type=checkbox id=__nav_4_4_6> <label class=md-nav__link for=__nav_4_4_6 tabindex=0 aria-expanded=false> Successful integrations <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Successful integrations" data-md-level=3> <label class=md-nav__title for=__nav_4_4_6> <span class="md-nav__icon md-icon"></span> Successful integrations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/particlenet.html class=md-nav__link> ParticleNet </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_5 type=checkbox id=__nav_4_5> <label class=md-nav__link for=__nav_4_5 tabindex=0 aria-expanded=false> Training <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Training data-md-level=2> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> Training </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../training/Decorrelation.html class=md-nav__link> Decorrelation </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_4_5_2 type=checkbox id=__nav_4_5_2> <label class=md-nav__link for=__nav_4_5_2 tabindex=0 aria-expanded=false> Training as a Service <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Training as a Service" data-md-level=3> <label class=md-nav__title for=__nav_4_5_2> <span class="md-nav__icon md-icon"></span> Training as a Service </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../training/MLaaS4HEP.html class=md-nav__link> MLaaS4HEP </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../training/autoencoders.html class=md-nav__link> Autoencoders </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-we-talk-about-when-we-talk-about-model-optimization class=md-nav__link> What we talk about when we talk about model optimization </a> <nav class=md-nav aria-label="What we talk about when we talk about model optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#caveat-which-data-should-you-use-to-optimize-your-model class=md-nav__link> Caveat: which data should you use to optimize your model </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#grid-search class=md-nav__link> Grid Search </a> </li> <li class=md-nav__item> <a href=#random-search class=md-nav__link> Random search </a> </li> <li class=md-nav__item> <a href=#model-based-optimization-by-gradient-descent class=md-nav__link> Model-based optimization by gradient descent </a> <nav class=md-nav aria-label="Model-based optimization by gradient descent"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-based-optimization-by-surrogates class=md-nav__link> Model-based optimization by surrogates </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bayesian-optimization class=md-nav__link> Bayesian Optimization </a> <nav class=md-nav aria-label="Bayesian Optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gaussian-processes class=md-nav__link> Gaussian processes </a> </li> <li class=md-nav__item> <a href=#the-basic-idea-behind-bayesian-optimization class=md-nav__link> The basic idea behind Bayesian optimization </a> <nav class=md-nav aria-label="The basic idea behind Bayesian optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#historical-note class=md-nav__link> Historical note </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bayesian-optimization-in-practice class=md-nav__link> Bayesian optimization in practice </a> </li> <li class=md-nav__item> <a href=#limitations-and-some-workaround-of-bayesian-optimization class=md-nav__link> Limitations (and some workaround) of Bayesian Optimization </a> </li> <li class=md-nav__item> <a href=#alternatives-to-gaussian-processes-tree-based-models class=md-nav__link> Alternatives to Gaussian processes: Tree-based models </a> </li> <li class=md-nav__item> <a href=#implementations-of-bayesian-optimization class=md-nav__link> Implementations of Bayesian Optimization </a> </li> <li class=md-nav__item> <a href=#caveats-dont-get-too-obsessed-with-model-optimization class=md-nav__link> Caveats: don't get too obsessed with model optimization </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=model-optimization>Model optimization<a class=headerlink href=#model-optimization title="Permanent link">&para;</a></h1> <p>This page summarizes the concepts shown in <a href=https://indico.cern.ch/event/1084800/#4-bayesian-optimization>a contribution on Bayesian Optimization to the ML Forum</a> and may be edited and published elsewhere by the author.</p> <h2 id=what-we-talk-about-when-we-talk-about-model-optimization>What we talk about when we talk about model optimization<a class=headerlink href=#what-we-talk-about-when-we-talk-about-model-optimization title="Permanent link">&para;</a></h2> <p>Given some data <span class=arithmatex>\(x\)</span> and a family of functionals parameterized by (a vector of) parameters <span class=arithmatex>\(\theta\)</span> (e.g. for DNN training weights), the problem of learning consists in finding <span class=arithmatex>\(argmin_\theta Loss(f_\theta(x) - y_{true})\)</span>. The treatment below focusses on gradient descent, but the formalization is completely general, i.e. it can be applied also to methods that are not explicitly formulated in terms of gradient descent (e.g. BDTs). The mathematical formalism for the problem of learning is briefly explained in <a href=https://indico.cern.ch/event/1036246/#5-statistics-statistical-learn>a contribution on statistical learning to the ML forum</a>: for the purposes of this documentation we will proceed through two illustrations.</p> <p>The first illustration, elaborated from an image by <a href=https://forum.huawei.com/enterprise/en/machine-learning-training-method-gradient-descent-method/thread/708303-895>the huawei forums</a> shows the general idea behind learning through gradient descent in a multidimensional parameter space, where the minimum of a loss function is found by following the function's gradient until the minimum.</p> <figure> <img src=../images/optimization/gradientdescent.png width=60%> <figcaption>The cartoon illustrates the general idea behind gradient descent to find the minimum of a function in a multidimensional parameter space (figure elaborated from an image by <a href=https://forum.huawei.com/enterprise/en/machine-learning-training-method-gradient-descent-method/thread/708303-895>the huawei forums</a>).</figcaption> </figure> <p>The model to be optimized via a loss function typically is a parametric function, where the set of parameters (e.g. the <em>network weights</em> in neural networks) corresponds to a certain fixed structure of the network. For example, a network with two inputs, two inner layers of two neurons, and one output neuron will have six parameters whose values will be changed until the loss function reaches its minimum.</p> <p>When we talk about model optimization we refer to the fact that often we are interested in finding which model structure is the best to describe our data. The main concern is to design a model that has a sufficient complexity to store all the information contained in the training data. We can therefore think of parameterizing the network structure itself, e.g. in terms of the number of inner layers and number of neurons per layer: these <em>hyperparameters</em> define a space where we want to again minimize a loss function. Formally, the parametric function <span class=arithmatex>\(f_\theta\)</span> is also a function of these hyperparameters <span class=arithmatex>\(\lambda\)</span>: <span class=arithmatex>\(f_{(\theta, \lambda)}\)</span>, and the <span class=arithmatex>\(\lambda\)</span> can be optimized</p> <p>The second illustration, also elaborated from an image by <a href=https://forum.huawei.com/enterprise/en/machine-learning-training-method-gradient-descent-method/thread/708303-895>the huawei forums</a>, broadly illustrates this concept: for each point in the hyperparameters space (that is, for each configuration of the model), the individual model is optimized as usual. The global minimum over the hyperparameters space is then sought.</p> <figure> <img src=../images/optimization/gradientdescent.png width=60%> <figcaption>The cartoon illustrates the general idea behind gradient descent to optimize the model complexity (in terms of the choice of hyperparameters) multidimensional parameter and hyperparameter space (figure elaborated from an image by <a href=https://forum.huawei.com/enterprise/en/machine-learning-training-method-gradient-descent-method/thread/708303-895>the huawei forums</a>).</figcaption> </figure> <h4 id=caveat-which-data-should-you-use-to-optimize-your-model>Caveat: which data should you use to optimize your model<a class=headerlink href=#caveat-which-data-should-you-use-to-optimize-your-model title="Permanent link">&para;</a></h4> <p>In typical machine learning studies, you should divide your dataset into three parts. One is used for training the model (<em>training sample</em>), one is used for testing the performance of the model (<em>test sample</em>), and the third one is the one where you actually use your trained model, e.g. for inference (<em>application sample</em>). Sometimes you may get away with using test data as application data: Helge Voss (Chap 5 of Behnke et al.) states that this is acceptable under three conditions that must be simultaneously valid:</p> <ul> <li>no hyperparameter optimization is performed;</li> <li>no overtraining is found;</li> <li>the number of training data is high enough to make statistical fluctuations negligible.</li> </ul> <p>If you are doing any kind of hyperparamters optimization, <strong>thou shalt NOT use the test sample as application sample</strong>. You should have at least three distinct sets, and ideally you should use four (training, testing, hyperparameter optimization, application).</p> <h2 id=grid-search>Grid Search<a class=headerlink href=#grid-search title="Permanent link">&para;</a></h2> <p>The most simple hyperparameters optimization algorithm is the <em>grid search</em>, where you train all the models in the hyperparameters space to build the full landscape of the global loss function, as illustrated in <a href=https://www.deeplearningbook.org/ >Goodfellow, Bengio, Courville: "Deep Learning"</a>.</p> <figure> <img src=../images/optimization/grid.png width=60%> <figcaption>The cartoon illustrates the general idea behind grid search (image taken from <a href=https://www.deeplearningbook.org>Goodfellow, Bengio, Courville: "Deep Learning"</a>).</figcaption> </figure> <p>To perform a meaningful grid search, you have to provide a set of values within the acceptable range of each hyperparameters, then for each point in the cross-product space you have to train the corresponding model.</p> <p>The main issue with grid search is that when there are nonimportant hyperparameters (i.e. hyperparameters whose value doesn't influence much the model performance) the algorithm spends an exponentially large time (in the number of nonimportant hyperparameters) in the noninteresting configurations: having <span class=arithmatex>\(m\)</span> parameters and testing <span class=arithmatex>\(n\)</span> values for each of them leads to <span class=arithmatex>\(\mathcal{O}(n^m)\)</span> tested configurations. While the issue may be mitigated by parallelization, when the number of hyperparameters (the dimension of hyperparameters space) surpasses a handful, even parallelization can't help.</p> <p>Another issue is that the search is binned: depending on the granularity in the scan, the global minimum may be invisible.</p> <p>Despite these issues, grid search is sometimes still a feasible choice, and gives its best when done <strong>iteratively</strong>. For example, if you start from the interval <span class=arithmatex>\(\{-1, 0, 1\}\)</span>:</p> <ul> <li>if the best parameter is found to be at the boundary (1), then extend range (<span class=arithmatex>\(\{1, 2, 3\}\)</span>) and do the search in the new range;</li> <li>if the best parameter is e.g. at 0, then maybe zoom in and do a search in the range <span class=arithmatex>\(\{-0.1, 0, 0.1\}\)</span>.</li> </ul> <h2 id=random-search>Random search<a class=headerlink href=#random-search title="Permanent link">&para;</a></h2> <p>An improvement of the grid search is the <em>random search</em>, which proceeds like this:</p> <ul> <li>you provide a marginal p.d.f. for each hyperparameter;</li> <li>you sample from the joint p.d.f. a certain number of training configurations;</li> <li>you train for each of these configurations to build the loss function landscape.</li> </ul> <p>This procedure has significant advantages over a simple grid search: random search is not binned, because you are sampling from a continuous p.d.f., so the pool of explorable hyperparameter values is larger; random search is exponentially more efficient, because it tests a unique value for each influential hyperparameter on nearly every trial.</p> <p>Random search also work best when done iteratively. The differences between grid and random search are again illustrated in <a href=https://www.deeplearningbook.org/ >Goodfellow, Bengio, Courville: "Deep Learning"</a>.</p> <figure> <img src=../images/optimization/gridrandom.png width=60%> <figcaption>The cartoon illustrates the general idea behind random search, as opposed to grid search (image taken from <a href=https://www.deeplearningbook.org/ >Goodfellow, Bengio, Courville: "Deep Learning"</a>).</figcaption> </figure> <h2 id=model-based-optimization-by-gradient-descent>Model-based optimization by gradient descent<a class=headerlink href=#model-based-optimization-by-gradient-descent title="Permanent link">&para;</a></h2> <p>Now that we have looked at the most basic model optimization techniques, we are ready to look into using gradient descent to solve a model optimization problem. We will proceed by recasting the problem as one of <em>model selection</em>, where the hyperparameters are the input (decision) variables, and the model selection criterion is a differentiable validation set error. The validation set error attempts to describe the complexity of the network by a single hyperparameter (details in [<a href=https://indico.cern.ch/event/1036246/#5-statistics-statistical-learn>a contribution on statistical learning to the ML forum</a>]) The problem may be solved with standard gradient descent, as illustrated above, if we assume that the training criterion <span class=arithmatex>\(C\)</span> is continuous and differentiable with respect to both the parameters <span class=arithmatex>\(\theta\)</span> (e.g. weights) and hyperparameters <span class=arithmatex>\(\lambda\)</span> Unfortunately, the gradient is seldom available (either because it has a prohibitive computational cost, or because it is non-differentiable as is the case when there are discrete variables).</p> <p>A diagram illustrating the way gradient-based model optimization works has been prepared by <a href=https://doi.org/10.1162/089976600300015187>Bengio, doi:10.1162/089976600300015187</a>.</p> <figure> <img src=../images/optimization/modelbased.png width=60%> <figcaption>The diagram illustrates the way model optimization can be recast as a model selection problem, where a model selection criterion involves a differentiable validation set error (image taken from <a href=https://doi.org/10.1162/089976600300015187>Bengio, doi:10.1162/089976600300015187</a>).</figcaption> </figure> <h3 id=model-based-optimization-by-surrogates>Model-based optimization by surrogates<a class=headerlink href=#model-based-optimization-by-surrogates title="Permanent link">&para;</a></h3> <p>Sequential Model-based Global Optimization (SMBO) consists in replacing the loss function with a surrogate model of it, when the loss function (i.e. the validation set error) is not available. The surrogate is typically built as a Bayesian regression model, when one estimates the expected value of the validation set error for each hyperparameter <strong>together with the uncertainty in this expectation</strong>. The pseudocode for the SMBO algorithm is illustrated by <a href=https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf>Bergstra et al</a>.</p> <figure> <img src=../images/optimization/smbo.png width=60%> <figcaption>The diagram illustrates the pseudocode for the Sequential Model-based Global Optimization (image taken from <a href=https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf>Bergstra et al</a>).</figcaption> </figure> <p>This procedure results in a tradeoff between: <strong>exploration</strong>, i.e. proposing hyperparameters with high uncertainty, which may result in substantial improvement or not; and <strong>exploitation</strong> (propose hyperparameters that will likely perform as well as the current proposal---usually this mean close to the current ones). The disadvantage is that the whole procedure must run until completion before giving as an output any usable information. By comparison, manual or random searches tend to give hints on the location of the minimum faster.</p> <h2 id=bayesian-optimization>Bayesian Optimization<a class=headerlink href=#bayesian-optimization title="Permanent link">&para;</a></h2> <p>We are now ready to tackle in full what is referred to as <em>Bayesian optimization</em>.</p> <p>Bayesian optimization assumes that the unknown function <span class=arithmatex>\(f(\theta, \lambda)\)</span> was sampled from a Gaussian process (GP), and that after the observations it maintains the corresponding posterior. In this context, <em>observations</em> are the various validation set errors for different values of the hyperparameters <span class=arithmatex>\(\lambda\)</span>. In order to pick the next value to probe, one maximizes some estimate of the expected improvement (see below). To understand the meaning of "sampled from a Gaussian process", we need to define what a Gaussian process is.</p> <h4 id=gaussian-processes>Gaussian processes<a class=headerlink href=#gaussian-processes title="Permanent link">&para;</a></h4> <p>Gaussian processes (GPs) generalize the concept of <em>Gaussian distribution over discrete random variables</em> to the concept of <em>Gaussian distribution over continuous functions</em>. Given some data and an estimate of the Gaussian noise, by fitting a function one can estimate also the noise at the interpolated points. This estimate is made by similarity with contiguous points, adjusted by the distance between points. A GP is therefore fully described by its mean and its covariance function. An illustration of Gaussian processes is given in <a href=https://courses.cs.washington.edu/courses/cse599i/18wi/ >Kevin Jamieson's CSE599 lecture notes</a>.</p> <figure> <img src=../images/optimization/gausproc.png width=60%> <figcaption>The diagram illustrates the evolution of a Gaussian process, when adding interpolating points (image taken from <a href=https://courses.cs.washington.edu/courses/cse599i/18wi/ >Kevin Jamieson's CSE599 lecture notes</a>).</figcaption> </figure> <p>GPs are great for Bayesian optimization because they out-of-the-box provide the expected value (i.e. the mean of the process) and its uncertainty (covariance function).</p> <h3 id=the-basic-idea-behind-bayesian-optimization>The basic idea behind Bayesian optimization<a class=headerlink href=#the-basic-idea-behind-bayesian-optimization title="Permanent link">&para;</a></h3> <p>Gradient descent methods are intrinsically local: the decision on the next step is taken based on the local gradient and Hessian approximations- Bayesian optimization (BO) with GP priors uses a model that uses all the information from the previous steps by encoding it in the model giving the expectation and its uncertainty. The consequence is that GP-based BO can find the minimum of difficult nonconvex functions in relatively few evaluations, at the cost of performing more computations to find the next point to try in the hyperparameters space.</p> <p>The BO prior is a prior over the space of the functions. GPs are especially suited to play the role of BO prior, because marginals and conditionals can be computed in closed form (thanks to the properties of the Gaussian distribution).</p> <p>There are several methods to choose the acquisition function (the function that selects the next step for the algorithm), but there is no omnipurpose recipe: the best approach is problem-dependent. The acquisition function involves an accessory optimization to maximize a certain quantity; typical choices are:</p> <ul> <li>maximize the probability of improvement over the current best value: can be calculated analytically for a GP;</li> <li>maximize the expected improvement over the current best value: can also be calculated analytically for a GP;</li> <li>maximize the GP Upper confidence bound: minimize "regret" over the course of the optimization.</li> </ul> <h4 id=historical-note>Historical note<a class=headerlink href=#historical-note title="Permanent link">&para;</a></h4> <p>Gaussian process regression is also called <em>kriging</em> in geostatistics, after Daniel G. Krige (1951) who pioneered the concept later formalized by Matheron (1962)</p> <h2 id=bayesian-optimization-in-practice>Bayesian optimization in practice<a class=headerlink href=#bayesian-optimization-in-practice title="Permanent link">&para;</a></h2> <p>The figure below, taken by <a href=http://krasserm.github.io/2018/03/21/bayesian-optimization/ >a tutorial on BO by Martin Krasser</a>, clarifies rather well the procedure. The task is to approximate the target function (labelled <em>noise free objective</em> in the figure), given some noisy samples of it (the black crosses). At the first iteration, one starts from a flat surrogate function, with a given uncertainty, and fits it to the noisy samples. To choose the next sampling location, a certain acquisition function is computed, and the value that maximizes it is chosen as the next sampling location At each iteration, more noisy samples are added, until the distance between consecutive sampling locations is minimized (or, equivalently, a measure of the value of the best selected sample is maximized).</p> <figure> <img src=../images/optimization/bo_1.png width=60%> </figure> <figure> <img src=../images/optimization/bo_2.png width=60%> </figure> <figure> <img src=../images/optimization/bo_3.png width=60%> </figure> <figure> <img src=../images/optimization/bo_4.png width=60%> <figcaption>Practical illustration of Bayesian Optimization (images taken from <a href=http://krasserm.github.io/2018/03/21/bayesian-optimization/ >a tutorial on BO by Martin Krasser]</a>).</figcaption> </figure> <h2 id=limitations-and-some-workaround-of-bayesian-optimization>Limitations (and some workaround) of Bayesian Optimization<a class=headerlink href=#limitations-and-some-workaround-of-bayesian-optimization title="Permanent link">&para;</a></h2> <p>There are three main limitations to the BO approach. A good overview of these limitations and of possible solutions can be found in <a href=https://arxiv.org/abs/1206.2944>arXiv:1206.2944</a>.</p> <p>First of all, it is unclear what is an appropriate choice for the covariance function and its associated hyperparameters. In particular, the standard squared exponential kernel is often too smooth. As a workaround, alternative kernels may be used: a common choice is the Mat&eacute;rn 5/2 kernel, which is similar to the squared exponential one but allows for non-smoothness.</p> <p>Another issue is that, for certain problems, the function evaluation may take very long to compute. To overcome this, often one can replace the function evaluation with the Monte Carlo integration of the expected improvement over the GP hyperparameters, which is faster.</p> <p>The third main issue is that for complex problems one would ideally like to take advantage of parallel computation. The procedure is iterative, however, and it is not easy to come up with a scheme to make it parallelizable. The referenced paper proposed sampling over the expected acquisition, <em>conditioned on all the pending evaluations</em>: this is computationally cheap and is intrinsically parallelizable.</p> <h2 id=alternatives-to-gaussian-processes-tree-based-models>Alternatives to Gaussian processes: Tree-based models<a class=headerlink href=#alternatives-to-gaussian-processes-tree-based-models title="Permanent link">&para;</a></h2> <p>Gaussian Processes model directly <span class=arithmatex>\(P(hyperpar | data)\)</span> but are not the only suitable surrogate models for Bayesian optimization</p> <p>The so-called <em>Tree-structured Parzen Estimator</em> (TPE), described in <a href=https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf>Bergstra et al</a>, models separately <span class=arithmatex>\(P(data | hyperpar)\)</span> and <span class=arithmatex>\(P(hyperpar)\)</span>, to then obtain the posterior by explicit application of the Bayes theorem TPEs exploit the fact that the choice of hyperparameters is intrinsically graph-structured, in the sense that e.g. you first choose the number of layers, then choose neurons per layer, etc. TPEs run over this generative process by replacing the hyperparameters priors with nonparametric densities. These generative nonparametric densities are built by classifying them into those that result in worse/better loss than the current proposal.</p> <p>TPEs have been used in CMS already around 2017 in a VHbb analysis (see <a href=https://github.com/swang373/vhbb_optimization>repository by Sean-Jiun Wang</a>) and in a charged Higgs to tb search (<a href=http://dx.doi.org/10.1007/JHEP01(2020)096>HIG-18-004, doi:10.1007/JHEP01(2020)096</a>).</p> <h2 id=implementations-of-bayesian-optimization>Implementations of Bayesian Optimization<a class=headerlink href=#implementations-of-bayesian-optimization title="Permanent link">&para;</a></h2> <ul> <li><strong>Implementations in R</strong> are readily available <a href=https://tensorflow.rstudio.com/tools/tfruns/tuning/ >as the R-studio <code>tuning</code> package</a>;</li> <li><strong>Scikit-learn</strong> provides a <a href=\https://scikit-learn.org/stable/modules/gaussian_process.html>handy implementation of Gaussian processes</a>;</li> <li>**scipy* provides a handy implementation of the optimization routines;</li> <li><code>hyperopt</code> provides a handy implementation of distributed <a href=https://github.com/hyperopt/hyperopt>hyperparameter optimization routines</a>;<ul> <li>GPs not coded by default, hence must rely on scikit-learn;</li> <li>Parzen tree estimators are implemented by default (together with random search);</li> </ul> </li> <li>Several handy tutorials online focussed on hyperparameters optimization<ul> <li><a href=http://krasserm.github.io/2018/03/21/bayesian-optimization/ >Tutorial by Martin Krasser</a>;</li> <li><a href=https://machinelearningmastery.com/what-is-bayesian-optimization/ >Tutorial by Jason Brownlee</a>;</li> </ul> </li> <li>Early example of <code>hyperopt</code> in CMS<ul> <li>VHbb analysis: <a href=https://github.com/swang373/vhbb_optimization>repository by Sean-Jiun Wang</a>), for optimization of a BDT;</li> <li>Charged Higgs <a href=http://dx.doi.org/10.1007/JHEP01(2020)096>HIG-18-004, doi:10.1007/JHEP01(2020)096</a>) for optimization of a DNN (no public link for the code, contact me if needed)</li> </ul> </li> <li>Several expansions and improvements (particularly targeted at HPC clusters) are available, see e.g. <a href=https://indico.cern.ch/event/1084800/#3-hyper-parameter-tuning-of-th>this talk by Eric Wulff</a>.</li> </ul> <h2 id=caveats-dont-get-too-obsessed-with-model-optimization>Caveats: don't get too obsessed with model optimization<a class=headerlink href=#caveats-dont-get-too-obsessed-with-model-optimization title="Permanent link">&para;</a></h2> <p>In general, optimizing model structure is a good thing. F. Chollet e.g. says <em>"If you want to get to the very limit of what can be achieved on a given task, you can't be content with arbitrary choices made by a fallible human"</em>. On the other side, for many problems hyperparameter optimization does result in small improvements, and there is a tradeoff between improvement and time spent on the task: sometimes the time spent on optimization may not be worth, e.g. when the gradient of the loss in hyperparameters space is very flat (i.e. different hyperparameter sets give more or less the same results), particularly if you already know that small improvements will be eaten up by e.g. systematic uncertainties. On the other side, before you perform the optimization you don't know if the landscape is flat or if you can expect substantial improvements. Sometimes broad grid or random searches may give you a hint on whether the landscape of hyperparameters space is flat or not.</p> <p>Sometimes you may get good (and faster) improvements by <em>model ensembling</em> rather than by model optimization. To do model ensembling, you first train a handful models (either different methods---BDT, SVM, NN, etc---or different hyperparameters sets): <span class=arithmatex>\(pred\_a = model\_a.predict(x)\)</span>, ..., <span class=arithmatex>\(pred\_d = model\_d.predict(x)\)</span>. You then pool the predictions: <span class=arithmatex>\(pooled\_pred = (pred\_a + pred\_b + pred\_c + pred\_d)/4.\)</span>. THis works if all models are kind of good: if one is significantly worse than the others, then <span class=arithmatex>\(pooled\_pred\)</span> may not be as good as the best model of the pool.</p> <p>You can also find ways of ensembling in a smarter way, e.g. by doing weighted rather than simple averages: <span class=arithmatex>\(pooled\_pred = 0.5\cdot pred\_a + 0.25\cdot pred\_b + 0.1\cdot pred\_c + 0.15\cdot pred\_d)/4.\)</span>. Here the idea is to give more weight to better classifiers. However, you transfer the problem to having to choose the weights. These can be found empirically empirically by using random search or other algorithms like Nelder-Mead (<code>result = scipy.optimize.minimize(objective, pt, method='nelder-mead'</code>), where you build simplexes (polytope with <code>N+1</code> vertices in <code>N</code> dimensions, generalization of triangle) and stretch them towards higher values of the objective. Nelder-Mead can converge to nonstationary points, but there are extensions of the algorithm that may help.</p> <hr> <p>This page summarizes the concepts shown in <a href=https://indico.cern.ch/event/1084800/#4-bayesian-optimization>a contribution on Bayesian Optimization to the ML Forum</a>. Content may be edited and published elsewhere by the author. Page author: Pietro Vischia, 2022</p> <hr> <div class=md-source-file> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">January 8, 2024</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2020-2023 CMS Machine Learning Group </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/cms-ml target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 480 512"><!-- Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg> </a> <a href=https://hub.docker.com/orgs/cmsml/repositories target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1v59.4zm0-204.3h-66.1v60.7h66.1V32zm78.2 144.8H362v59.4h66.1v-59.4zm-156.3-72.1h-66.1v60.1h66.1v-60.1zm78.1 0h-66.1v60.1h66.1v-60.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1l-13.3-8.9zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1v-59.4zm-78.1-72.1h-66.1v60.1h66.1v-60.1z"/></svg> </a> <a href=https://cms-talk.web.cern.ch/c/physics/ml/104 target=_blank rel=noopener title=cms-talk.web.cern.ch class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M256 448c141.4 0 256-93.1 256-208S397.4 32 256 32 0 125.1 0 240c0 45.1 17.7 86.8 47.7 120.9-1.9 24.5-11.4 46.3-21.4 62.9-5.5 9.2-11.1 16.6-15.2 21.6-2.1 2.5-3.7 4.4-4.9 5.7-.6.6-1 1.1-1.3 1.4l-.3.3c-4.6 4.6-5.9 11.4-3.4 17.4 2.5 6 8.3 9.9 14.8 9.9 28.7 0 57.6-8.9 81.6-19.3 22.9-10 42.4-21.9 54.3-30.6 31.8 11.5 67 17.9 104.1 17.9zM128 272c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm128 0c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm160-32c0 17.7-14.3 32-32 32s-32-14.3-32-32 14.3-32 32-32 32 14.3 32 32z"/></svg> </a> <a href=mailto:cms-conveners-ml-knowledge@cern.ch target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l217.6 163.2c11.4 8.5 27 8.5 38.4 0l217.6-163.2c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48H48zM0 176v208c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V176L294.4 339.2a63.9 63.9 0 0 1-76.8 0L0 176z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["instant", "navigation.sections"], "search": "../assets/javascripts/workers/search.e5c33ebb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../assets/javascripts/bundle.ba449ae6.min.js></script> <script src=https://unpkg.com/mermaid@9.3/dist/mermaid.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>