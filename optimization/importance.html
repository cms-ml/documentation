<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Documentation of the CMS Machine Learning Group"><link rel=canonical href=https://cms-ml.github.io/documentation/optimization/importance.html><meta name=author content="CMS Machine Learning Group"><link rel="shortcut icon" href=../images/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-5.5.3"><title>Feature importance - CMS Machine Learning Documentation</title><link rel=stylesheet href=../assets/stylesheets/main.947af8d5.min.css><link rel=stylesheet href=../assets/stylesheets/palette.7f672a1f.min.css><meta name=theme-color content=#3f51b5><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style></head> <body dir=ltr data-md-color-scheme=preference data-md-color-primary=indigo data-md-color-accent=orange> <script>matchMedia("(prefers-color-scheme: dark)").matches&&document.body.setAttribute("data-md-color-scheme","slate")</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#feature-importance class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://cms-ml.github.io/documentation title="CMS Machine Learning Documentation" class="md-header-nav__button md-logo" aria-label="CMS Machine Learning Documentation"> <img src=../images/logo.png alt=logo> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> CMS Machine Learning Documentation </span> <span class="md-header-nav__topic md-ellipsis"> Feature importance </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/cms-ml/documentation/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> cms-ml/documentation </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://cms-ml.github.io/documentation title="CMS Machine Learning Documentation" class="md-nav__button md-logo" aria-label="CMS Machine Learning Documentation"> <img src=../images/logo.png alt=logo> </a> CMS Machine Learning Documentation </label> <div class=md-nav__source> <a href=https://github.com/cms-ml/documentation/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> cms-ml/documentation </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../index.html title=Home class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> Innovation <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Innovation data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Innovation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../innovation/journal_club.html title="ML Journal Club" class=md-nav__link> ML Journal Club </a> </li> <li class=md-nav__item> <a href=../innovation/hackathons.html title="ML Hackathons" class=md-nav__link> ML Hackathons </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Resources <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Resources data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Resources/Cloud_Resources/index.html title="Cloud Resources" class=md-nav__link> Cloud Resources </a> </li> <li class=md-nav__item> <a href=../Resources/FPGA_Resources/index.html title="FPGA Resource" class=md-nav__link> FPGA Resource </a> </li> <li class=md-nav__item> <a href=../Resources/GPU_Resources/index.html title="GPU Resources" class=md-nav__link> GPU Resources </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4 checked> <label class=md-nav__link for=nav-4> Tutorials <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Tutorials data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Tutorials </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-1 type=checkbox id=nav-4-1 checked> <label class=md-nav__link for=nav-4-1> Optimization <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Optimization data-md-level=2> <label class=md-nav__title for=nav-4-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Optimization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=model_optimization.html title="Model optimization" class=md-nav__link> Model optimization </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Feature importance <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg> </span> </label> <a href=importance.html title="Feature importance" class="md-nav__link md-nav__link--active"> Feature importance </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#general-discussion class=md-nav__link> General Discussion </a> <nav class=md-nav aria-label="General Discussion"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#filter-methods class=md-nav__link> Filter Methods </a> </li> <li class=md-nav__item> <a href=#embedded-methods class=md-nav__link> Embedded Methods </a> </li> <li class=md-nav__item> <a href=#wrapper-methods class=md-nav__link> Wrapper Methods </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#introduction-by-example class=md-nav__link> Introduction by Example </a> <nav class=md-nav aria-label="Introduction by Example"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#direct-interpretation class=md-nav__link> Direct Interpretation </a> </li> <li class=md-nav__item> <a href=#permutation-importance class=md-nav__link> Permutation Importance </a> </li> <li class=md-nav__item> <a href=#l1-enforced-sparsity class=md-nav__link> L1-Enforced Sparsity </a> </li> <li class=md-nav__item> <a href=#recursive-feature-elimination class=md-nav__link> Recursive Feature Elimination </a> </li> <li class=md-nav__item> <a href=#feature-correlations class=md-nav__link> Feature Correlations </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#feature-importance-in-decision-trees class=md-nav__link> Feature Importance in Decision Trees </a> <nav class=md-nav aria-label="Feature Importance in Decision Trees"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example class=md-nav__link> Example </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=data_augmentation.html title="Data augmentation" class=md-nav__link> Data augmentation </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-2 type=checkbox id=nav-4-2> <label class=md-nav__link for=nav-4-2> Validation <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Validation data-md-level=2> <label class=md-nav__title for=nav-4-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Validation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../validation/overtraining.html title=Overtraining class=md-nav__link> Overtraining </a> </li> <li class=md-nav__item> <a href=../validation/cross_validation.html title="Cross validation" class=md-nav__link> Cross validation </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-3 type=checkbox id=nav-4-3> <label class=md-nav__link for=nav-4-3> Inference <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Inference data-md-level=2> <label class=md-nav__title for=nav-4-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Inference </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-3-1 type=checkbox id=nav-4-3-1> <label class=md-nav__link for=nav-4-3-1> Direct inference <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Direct inference" data-md-level=3> <label class=md-nav__title for=nav-4-3-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Direct inference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/tensorflow1.html title="TensorFlow 1" class=md-nav__link> TensorFlow 1 </a> </li> <li class=md-nav__item> <a href=../inference/tensorflow2.html title="TensorFlow 2" class=md-nav__link> TensorFlow 2 </a> </li> <li class=md-nav__item> <a href=../inference/pytorch.html title=PyTorch class=md-nav__link> PyTorch </a> </li> <li class=md-nav__item> <a href=../inference/pyg.html title="PyTorch Geometric" class=md-nav__link> PyTorch Geometric </a> </li> <li class=md-nav__item> <a href=../inference/onnx.html title=ONNX class=md-nav__link> ONNX </a> </li> <li class=md-nav__item> <a href=../inference/xgboost.html title=XGBoost class=md-nav__link> XGBoost </a> </li> <li class=md-nav__item> <a href=../inference/hls4ml.html title=hls4ml class=md-nav__link> hls4ml </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-3-2 type=checkbox id=nav-4-3-2> <label class=md-nav__link for=nav-4-3-2> Inference as a service <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Inference as a service" data-md-level=3> <label class=md-nav__title for=nav-4-3-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Inference as a service </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/sonic_triton.html title=Sonic/Triton class=md-nav__link> Sonic/Triton </a> </li> <li class=md-nav__item> <a href=../inference/tfaas.html title=TFaaS class=md-nav__link> TFaaS </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-3-3 type=checkbox id=nav-4-3-3> <label class=md-nav__link for=nav-4-3-3> Non-standard workflows <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Non-standard workflows" data-md-level=3> <label class=md-nav__title for=nav-4-3-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Non-standard workflows </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/standalone.html title="Standalone framework" class=md-nav__link> Standalone framework </a> </li> <li class=md-nav__item> <a href=../inference/swan_aws.html title="SWAN + AWS" class=md-nav__link> SWAN + AWS </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../inference/checklist.html title="Integration checklist" class=md-nav__link> Integration checklist </a> </li> <li class=md-nav__item> <a href=../inference/performance.html title=Performance class=md-nav__link> Performance </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-3-6 type=checkbox id=nav-4-3-6> <label class=md-nav__link for=nav-4-3-6> Successful integrations <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Successful integrations" data-md-level=3> <label class=md-nav__title for=nav-4-3-6> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Successful integrations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/particlenet.html title=ParticleNet class=md-nav__link> ParticleNet </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-4 type=checkbox id=nav-4-4> <label class=md-nav__link for=nav-4-4> Training <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Training data-md-level=2> <label class=md-nav__title for=nav-4-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Training </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-4-1 type=checkbox id=nav-4-4-1> <label class=md-nav__link for=nav-4-4-1> Training as a Service <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Training as a Service" data-md-level=3> <label class=md-nav__title for=nav-4-4-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Training as a Service </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../training/MLaaS4HEP.html title=MLaaS4HEP class=md-nav__link> MLaaS4HEP </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#general-discussion class=md-nav__link> General Discussion </a> <nav class=md-nav aria-label="General Discussion"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#filter-methods class=md-nav__link> Filter Methods </a> </li> <li class=md-nav__item> <a href=#embedded-methods class=md-nav__link> Embedded Methods </a> </li> <li class=md-nav__item> <a href=#wrapper-methods class=md-nav__link> Wrapper Methods </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#introduction-by-example class=md-nav__link> Introduction by Example </a> <nav class=md-nav aria-label="Introduction by Example"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#direct-interpretation class=md-nav__link> Direct Interpretation </a> </li> <li class=md-nav__item> <a href=#permutation-importance class=md-nav__link> Permutation Importance </a> </li> <li class=md-nav__item> <a href=#l1-enforced-sparsity class=md-nav__link> L1-Enforced Sparsity </a> </li> <li class=md-nav__item> <a href=#recursive-feature-elimination class=md-nav__link> Recursive Feature Elimination </a> </li> <li class=md-nav__item> <a href=#feature-correlations class=md-nav__link> Feature Correlations </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#feature-importance-in-decision-trees class=md-nav__link> Feature Importance in Decision Trees </a> <nav class=md-nav aria-label="Feature Importance in Decision Trees"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example class=md-nav__link> Example </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <a href=https://github.com/cms-ml/documentation/blob/master/content/optimization/importance.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=feature-importance>Feature Importance<a class=headerlink href=#feature-importance title="Permanent link">&para;</a></h1> <p>Feature importance is the impact a specific input field has on a prediction model's output. In general, these impacts can range from no impact (i.e. a feature with no variance) to perfect correlation with the ouput. There are several reasons to consider feature importance: </p> <ul> <li>Important features can be used to create simplified models, e.g. to mitigate overfitting.</li> <li>Using only important features can reduce the latency and memory requirements of the model. </li> <li>The relative importance of a set of features can yield insight into the nature of an otherwise opaque model (improved interpretability). </li> <li>If a model is sensitive to noise, rejecting irrelevant inputs may improve its performance. </li> </ul> <p>In the following subsections, we detail several strategies for evaluating feature importance. We begin with a general discussion of feature importance at a high level before offering a code-based tutorial on some common techniques. We conclude with additional notes and comments in the last section. </p> <h2 id=general-discussion>General Discussion<a class=headerlink href=#general-discussion title="Permanent link">&para;</a></h2> <p>Most feature importance methods fall into one of three broad categories: filter methods, embedding methods, and wrapper methods. Here we give a brief overview of each category with relevant examples: </p> <h3 id=filter-methods>Filter Methods<a class=headerlink href=#filter-methods title="Permanent link">&para;</a></h3> <p>Filter methods do not rely on a specific model, instead considering features in the context of a given dataset. In this way, they may be considered to be pre-processing steps. In many cases, the goal of feature filtering is to reduce high dimensional data. However, these methods are also applicable to data exploration, wherein an analyst simply seeks to learn about a dataset without actually removing any features. This knowledge may help interpret the performance of a downstream predictive model. Relevant examples include, </p> <ul> <li> <p><strong>Domain Knowledge</strong>: Perhaps the most obvious strategy is to select features relevant to the domain of interest. </p> </li> <li> <p><strong>Variance Thresholding</strong>: One basic filtering strategy is to simply remove features with low variance. In the extreme case, features with zero variance do not vary from example to example, and will therefore have no impact on the model's final prediction. Likewise, features with variance below a given threshold may not affect a model's downstream performance.</p> </li> <li> <p><strong>Fisher Scoring</strong>: Fisher scoring can be used to rank features; the analyst would then select the highest scoring features as inputs to a subsequent model. </p> </li> <li> <p><strong>Correlations</strong>: Correlated features introduce a certain degree of redundancy to a dataset, so reducing the number of strongly correlated variables may not impact a model's downstream performance. </p> </li> </ul> <h3 id=embedded-methods>Embedded Methods<a class=headerlink href=#embedded-methods title="Permanent link">&para;</a></h3> <p>Embedded methods are specific to a prediction model and independent of the dataset. Examples:</p> <ul> <li><strong>L1 Regularization (LASSO)</strong>: L1 regularization directly penalizes large model weights. In the context of linear regression, for example, this amounts to enforcing sparsity in the output prediction; weights corresponding to less relevant features will be driven to 0, nullifying the feature's effect on the output. </li> </ul> <h3 id=wrapper-methods>Wrapper Methods<a class=headerlink href=#wrapper-methods title="Permanent link">&para;</a></h3> <p>Wrapper methods iterate on prediction models in the context of a given dataset. In general they may be computationally expensive when compared to filter methods. Examples:</p> <ul> <li><strong>Permutation Importance</strong>: Direct interpretation isn't always feasible, so other methods have been developed to inspect a feature's importance. One common and broadly-applicable method is to randomly shuffle a given feature's input values and test the degredation of model performance. This process allows us to measure <a href=https://scikit-learn.org/stable/modules/permutation_importance.html>permutation importance</a> as follows. First, fit a model (<span class=arithmatex>\(f\)</span>) to training data, yielding <span class=arithmatex>\(f(X_\mathrm{train})\)</span>, where <span class=arithmatex>\(X_\mathrm{train}\in\mathbb{R}^{n\times d}\)</span> for <span class=arithmatex>\(n\)</span> input examples with <span class=arithmatex>\(d\)</span> features. Next, measure the model's performance on testing data for some loss <span class=arithmatex>\(\mathcal{L}\)</span>, i.e. <span class=arithmatex>\(s=\mathcal{L}\big(f(X_\mathrm{test}), y_\mathrm{test}\big)\)</span>. For each feature <span class=arithmatex>\(j\in[1\ ..\ d]\)</span>, randomly shuffle the corresponding column in <span class=arithmatex>\(X_\mathrm{test}\)</span> to form <span class=arithmatex>\(X_\mathrm{test}^{(j)}\)</span>. Repeat this process <span class=arithmatex>\(K\)</span> times, so that for <span class=arithmatex>\(k\in [1\ ..\ K]\)</span> each random shuffling of feature column <span class=arithmatex>\(j\)</span> gives a corrupted input dataset <span class=arithmatex>\(X_\mathrm{test}^{(j,k)}\)</span>. Finally, define the permutation importance of feature <span class=arithmatex>\(j\)</span> as the difference between the un-corrupted validation score and average validation score over the corrupted <span class=arithmatex>\(X_\mathrm{test}^{(j,k)}\)</span> datasets: </li> </ul> <div class=arithmatex>\[\texttt{PI}_j = s - \frac{1}{K}\sum_{k=1}^{K} \mathcal{L}[f(X_\mathrm{test}^{(j,k)}), y_\mathrm{test}]\]</div> <ul> <li><strong>Recursive Feature Elimination (RFE)</strong>: Given a prediction model and test/train dataset splits with <span class=arithmatex>\(D\)</span> initial features, RFE returns the set of <span class=arithmatex>\(d &lt; D\)</span> features that maximize model performance. First, the model is trained on the full set of features. The importance of each feature is ranked depending on the model type (e.g. for regression, the slopes are a sufficient ranking measure; permutation importance may also be used). The least important feature is rejected and the model is retrained. This process is repeated until the most significant <span class=arithmatex>\(d\)</span> features remain. </li> </ul> <h2 id=introduction-by-example>Introduction by Example<a class=headerlink href=#introduction-by-example title="Permanent link">&para;</a></h2> <h3 id=direct-interpretation>Direct Interpretation<a class=headerlink href=#direct-interpretation title="Permanent link">&para;</a></h3> <p>Linear regression is particularly interpretable because the prediction coefficients themselves can be interpreted as a measure of feature importance. Here we will compare this direct interpretation to several model inspection techniques. In the following examples we use the <a href=https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html>Diabetes Dataset</a> available as a <a href=https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset>Scikit-learn toy dataset</a>. This dataset maps 10 biological markers to a 1-dimensional quantitative measure of diabetes progression: </p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_diabetes</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>

<span class=n>diabetes</span> <span class=o>=</span> <span class=n>load_diabetes</span><span class=p>()</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_val</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_val</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>diabetes</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>diabetes</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>331</span><span class=p>,</span><span class=mi>10</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>y_train</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>331</span><span class=p>,)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>X_val</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>111</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>y_val</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>111</span><span class=p>,)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>diabetes</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>,</span> <span class=s1>&#39;sex&#39;</span><span class=p>,</span> <span class=s1>&#39;bmi&#39;</span><span class=p>,</span> <span class=s1>&#39;bp&#39;</span><span class=p>,</span> <span class=s1>&#39;s1&#39;</span><span class=p>,</span> <span class=s1>&#39;s2&#39;</span><span class=p>,</span> <span class=s1>&#39;s3&#39;</span><span class=p>,</span> <span class=s1>&#39;s4&#39;</span><span class=p>,</span> <span class=s1>&#39;s5&#39;</span><span class=p>,</span> <span class=s1>&#39;s6&#39;</span><span class=p>]</span>
</code></pre></div> To begin, let's use Ridge Regression (L2-regularized linear regression) to model diabetes progression as a function of the input markers. The absolute value of a regression coefficient (slope) corresponding to a feature can be interpreted the impact of a feature on the final fit:</p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>Ridge</span>
<span class=kn>from</span> <span class=nn>sklearn.feature_selection</span> <span class=kn>import</span> <span class=n>RFE</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1e-2</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Initial model score: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=o>-</span><span class=nb>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)):</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>diabetes</span><span class=o>.</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=nb>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=n>i</span><span class=p>]))</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Initial</span> <span class=n>model</span> <span class=n>score</span><span class=p>:</span> <span class=mf>0.357</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bmi</span><span class=p>:</span> <span class=mf>592.253</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s5</span><span class=p>:</span> <span class=mf>580.078</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bp</span><span class=p>:</span> <span class=mf>297.258</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s1</span><span class=p>:</span> <span class=mf>252.425</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>sex</span><span class=p>:</span> <span class=mf>203.436</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s3</span><span class=p>:</span> <span class=mf>145.196</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s4</span><span class=p>:</span> <span class=mf>97.033</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>age</span><span class=p>:</span> <span class=mf>39.103</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s6</span><span class=p>:</span> <span class=mf>32.945</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s2</span><span class=p>:</span> <span class=mf>20.906</span>
</code></pre></div> These results indicate that the bmi and s5 fields have the largest impact on the output of this regression model, while age, s6, and s2 have the smallest. Further interpretation is subject to the nature of the input data (see <a href=https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py>Common Pitfalls in the Interpretation of Coefficients of Linear Models</a>). Note that scikit-learn has <a href=https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel>tools</a> available to faciliate feature selections. </p> <h3 id=permutation-importance>Permutation Importance<a class=headerlink href=#permutation-importance title="Permanent link">&para;</a></h3> <p>In the context of our ridge regression example, we can calculate the permutation importance of each feature as follows (based on <a href=https://scikit-learn.org/stable/modules/permutation_importance.html]>scikit-learn docs</a>):</p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.inspection</span> <span class=kn>import</span> <span class=n>permutation_importance</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1e-2</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Initial model score: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=n>r</span> <span class=o>=</span> <span class=n>permutation_importance</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>,</span> <span class=n>n_repeats</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=o>.</span><span class=n>argsort</span><span class=p>()[::</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>diabetes</span><span class=o>.</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>&lt;8</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot; +/- </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_std</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Initial</span> <span class=n>model</span> <span class=n>score</span><span class=p>:</span> <span class=mf>0.357</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s5</span>      <span class=mf>0.204</span> <span class=o>+/-</span> <span class=mf>0.050</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bmi</span>     <span class=mf>0.176</span> <span class=o>+/-</span> <span class=mf>0.048</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bp</span>      <span class=mf>0.088</span> <span class=o>+/-</span> <span class=mf>0.033</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>sex</span>     <span class=mf>0.056</span> <span class=o>+/-</span> <span class=mf>0.023</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s1</span>      <span class=mf>0.042</span> <span class=o>+/-</span> <span class=mf>0.031</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s4</span>      <span class=mf>0.003</span> <span class=o>+/-</span> <span class=mf>0.008</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s6</span>      <span class=mf>0.003</span> <span class=o>+/-</span> <span class=mf>0.003</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s3</span>      <span class=mf>0.002</span> <span class=o>+/-</span> <span class=mf>0.013</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s2</span>      <span class=mf>0.002</span> <span class=o>+/-</span> <span class=mf>0.003</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>age</span>     <span class=o>-</span><span class=mf>0.002</span> <span class=o>+/-</span> <span class=mf>0.004</span>
</code></pre></div> These results are roughly consistent with the direct interpretation of the linear regression parameters; s5 and bmi are the most permutation-important features. This is because both have significant permutation importance scores (0.204, 0.176) when compared to the initial model score (0.357), meaning their random permutations significantly degraded the model perforamnce. On the other hand, s2 and age have approximately no permutation importance, meaning that the model's performance was robust to random permutations of these features. </p> <h3 id=l1-enforced-sparsity>L1-Enforced Sparsity<a class=headerlink href=#l1-enforced-sparsity title="Permanent link">&para;</a></h3> <p>In some applications it may be useful to reject features with low importance. Models biased towards sparsity are one way to achieve this goal, as they are designed to ignore a subset of features with the least impact on the model's output. In the context of linear regression, sparsity can be enforced by imposing L1 regularization on the regression coefficients (LASSO regression):</p> <div class=arithmatex>\[\mathcal{L}_\mathrm{LASSO} = \frac{1}{2n}||y - Xw||^2_2 + \alpha||w||_1\]</div> <p>Depending on the strength of the regularization <span class=arithmatex>\((\alpha)\)</span>, this loss function is biased to zero-out features of low importance. In our diabetes regression example, </p> <p><div class=highlight><pre><span></span><code><span class=n>model</span> <span class=o>=</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1e-1</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Model score: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=o>-</span><span class=nb>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)):</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>diabetes</span><span class=o>.</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>}</span><span class=s1>: </span><span class=si>{</span><span class=nb>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=n>i</span><span class=p>])</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Model</span> <span class=n>score</span><span class=p>:</span> <span class=mf>0.355</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bmi</span><span class=p>:</span> <span class=mf>592.203</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s5</span><span class=p>:</span> <span class=mf>507.363</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bp</span><span class=p>:</span> <span class=mf>240.124</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s3</span><span class=p>:</span> <span class=mf>219.104</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>sex</span><span class=p>:</span> <span class=mf>129.784</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s2</span><span class=p>:</span> <span class=mf>47.628</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s1</span><span class=p>:</span> <span class=mf>41.641</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>age</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s4</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s6</span><span class=p>:</span> <span class=mf>0.000</span>
</code></pre></div> For this value of <span class=arithmatex>\(\alpha\)</span>, we see that the model has rejected the age, s4, and s6 features as unimportant (consistent with the permutation importance measures above) while achieving a similar model score as the previous ridge regression strategy. </p> <h3 id=recursive-feature-elimination>Recursive Feature Elimination<a class=headerlink href=#recursive-feature-elimination title="Permanent link">&para;</a></h3> <p>Another common strategy is recursive feature elimination (RFE). Though RFE can be used for regression applications as well, we turn our attention to a classification task for the sake of variety. The following discussions are based on the <a href=https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+Diagnostic>Breast Cancer Wisconsin Diagnostic Dataset</a>, which maps 30 numeric features corresponding to digitized breast mass images to a binary classification of benign or malignant. </p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_breast_cancer</span>
<span class=kn>from</span> <span class=nn>sklearn.svm</span> <span class=kn>import</span> <span class=n>SVC</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>StratifiedKFold</span>

<span class=n>data</span> <span class=o>=</span> <span class=n>load_breast_cancer</span><span class=p>()</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_val</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_val</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>data</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>data</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>426</span><span class=p>,</span> <span class=mi>30</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>y_train</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>426</span><span class=p>,)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>X_val</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>143</span><span class=p>,</span> <span class=mi>30</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>y_val</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>143</span><span class=p>,)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>breast_cancer</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>[</span><span class=s1>&#39;mean radius&#39;</span> <span class=s1>&#39;mean texture&#39;</span> <span class=s1>&#39;mean perimeter&#39;</span> <span class=s1>&#39;mean area&#39;</span> <span class=s1>&#39;mean smoothness&#39;</span> <span class=s1>&#39;mean compactness&#39;</span> <span class=s1>&#39;mean concavity&#39;</span> <span class=s1>&#39;mean concave points&#39;</span> <span class=s1>&#39;mean symmetry&#39;</span> <span class=s1>&#39;mean fractal dimension&#39;</span> <span class=s1>&#39;radius error&#39;</span> <span class=s1>&#39;texture error&#39;</span> <span class=s1>&#39;perimeter error&#39;</span> <span class=s1>&#39;area error&#39;</span> <span class=s1>&#39;smoothness error&#39;</span> <span class=s1>&#39;compactness error&#39;</span> <span class=s1>&#39;concavity error&#39;</span> <span class=s1>&#39;concave points error&#39;</span> <span class=s1>&#39;symmetry error&#39;</span> <span class=s1>&#39;fractal dimension error&#39;</span> <span class=s1>&#39;worst radius&#39;</span> <span class=s1>&#39;worst texture&#39;</span> <span class=s1>&#39;worst perimeter&#39;</span> <span class=s1>&#39;worst area&#39;</span> <span class=s1>&#39;worst smoothness&#39;</span> <span class=s1>&#39;worst compactness&#39;</span> <span class=s1>&#39;worst concavity&#39;</span> <span class=s1>&#39;worst concave points&#39;</span> <span class=s1>&#39;worst symmetry&#39;</span> <span class=s1>&#39;worst fractal dimension&#39;</span><span class=p>]</span>
</code></pre></div> <p>Given a classifier and a classification task, recursive feature elimination (RFE, <a href=https://link.springer.com/content/pdf/10.1023/A:1012487302797.pdf>see original paper</a>) is the process of identifying the subset of input features leading to the most performative model. Here we employ a support vector machine classifier (SVM) with a linear kernel to perform binary classification on the input data. We ask for the top <span class=arithmatex>\(j\in[1\ .. \ d]\)</span> most important features in a for loop, computing the classification accuracy when only these features are leveraged. </p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.feature_selection</span> <span class=kn>import</span> <span class=n>RFE</span>

<span class=n>features</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>breast_cancer</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=n>svc</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;linear&#39;</span><span class=p>)</span>
<span class=k>for</span> <span class=n>n_features</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>30</span><span class=p>,</span> <span class=mi>1</span><span class=p>):</span>
    <span class=n>rfe</span> <span class=o>=</span> <span class=n>RFE</span><span class=p>(</span><span class=n>estimator</span><span class=o>=</span><span class=n>svc</span><span class=p>,</span> <span class=n>step</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_features_to_select</span><span class=o>=</span><span class=n>n_features</span><span class=p>)</span>
    <span class=n>rfe</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;n_features=</span><span class=si>{</span><span class=n>n_features</span><span class=si>}</span><span class=s1>, accuracy=</span><span class=si>{</span><span class=n>rfe</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39; - selected: </span><span class=si>{</span><span class=n>features</span><span class=p>[</span><span class=n>rfe</span><span class=o>.</span><span class=n>support_</span><span class=p>]</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.881</span>
<span class=o>&gt;&gt;&gt;</span>  <span class=o>-</span> <span class=n>selected</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;worst concave points&#39;</span><span class=p>]</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.874</span>
<span class=o>&gt;&gt;&gt;</span>  <span class=o>-</span> <span class=n>selected</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;worst concavity&#39;</span> <span class=s1>&#39;worst concave points&#39;</span><span class=p>]</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.867</span>
<span class=o>&gt;&gt;&gt;</span>  <span class=o>-</span> <span class=n>selected</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;mean concave points&#39;</span> <span class=s1>&#39;worst concavity&#39;</span> <span class=s1>&#39;worst concave points&#39;</span><span class=p>]</span>
 <span class=o>...</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.930</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>17</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.965</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>18</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.951</span>
<span class=o>...</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>27</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.958</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>28</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.958</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>29</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.958</span>
</code></pre></div> Here we've shown a subset of the output. In the first output lines, we see that the 'worst concave points' feature alone leads to 88.1% accuracy. Including the next two most important features actually degrades the classification accuracy. We then skip to the top 17 features, which in this case we observe to yield the best performance for the linear SVM classifier. The addition of more features does not lead to additional perforamnce boosts. In this way, RFE can be treated as a model wrapper introducing an additional hyperparameter, n_features_to_select, which can be used to optimize model performance. A more principled optimization using k-fold cross validation with RFE is available in the <a href=https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py>scikit-learn docs</a>. </p> <h3 id=feature-correlations>Feature Correlations<a class=headerlink href=#feature-correlations title="Permanent link">&para;</a></h3> <p>In the above, we have focused specifically on interpreting the importance of single features. However, it may be that several features are correlated, sharing the responsibility for the overall prediction of the model. In this case, some measures of feature importance may inappropriately downweight correlated features in a so-called correlation bias (see <a href=https://pubmed.ncbi.nlm.nih.gov/21576180/ >Classification with Correlated Features: Unrelability of Feature Ranking and Solutions</a>). For example, the permutation invariance of <span class=arithmatex>\(d\)</span> correlated features is shown to decrease (as a function of correlation strength) faster for higher <span class=arithmatex>\(d\)</span> (see <a href=https://link.springer.com/article/10.1007/s11222-016-9646-1>Correlation and Variable importance in Random Forests</a>). </p> <p>We can see these effects in action using the breast cancer dataset, following the corresponding <a href=https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py>scikit-learn example</a></p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.ensemble</span> <span class=kn>import</span> <span class=n>RandomForestClassifier</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
<span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_breast_cancer</span>

<span class=n>data</span> <span class=o>=</span> <span class=n>load_breast_cancer</span><span class=p>()</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>data</span><span class=o>.</span><span class=n>target</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=n>clf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>clf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Accuracy on test data: </span><span class=si>{:.2f}</span><span class=s2>&quot;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>clf</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)))</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Accuracy</span> <span class=n>on</span> <span class=n>test</span> <span class=n>data</span><span class=p>:</span> <span class=mf>0.97</span>
</code></pre></div> Here we've implemented a random forest classifier and achieved a high accuracy (97%) on the benign vs. malignent predictions. The permutation importances for the 10 most important training features are:</p> <div class=highlight><pre><span></span><code><span class=n>r</span> <span class=o>=</span> <span class=n>permutation_importance</span><span class=p>(</span><span class=n>clf</span><span class=p>,</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>n_repeats</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=o>.</span><span class=n>argsort</span><span class=p>()[::</span><span class=o>-</span><span class=mi>1</span><span class=p>][:</span><span class=mi>10</span><span class=p>]:</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>breast_cancer</span><span class=o>.</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>&lt;8</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot;  </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.5f</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot; +/- </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_std</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.5f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>worst</span> <span class=n>concave</span> <span class=n>points</span>  <span class=mf>0.00681</span> <span class=o>+/-</span> <span class=mf>0.00305</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>mean</span> <span class=n>concave</span> <span class=n>points</span>  <span class=mf>0.00329</span> <span class=o>+/-</span> <span class=mf>0.00188</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>worst</span> <span class=n>texture</span>  <span class=mf>0.00258</span> <span class=o>+/-</span> <span class=mf>0.00070</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>radius</span> <span class=n>error</span>  <span class=mf>0.00235</span> <span class=o>+/-</span> <span class=mf>0.00000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>mean</span> <span class=n>texture</span>  <span class=mf>0.00188</span> <span class=o>+/-</span> <span class=mf>0.00094</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>mean</span> <span class=n>compactness</span>  <span class=mf>0.00188</span> <span class=o>+/-</span> <span class=mf>0.00094</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>area</span> <span class=n>error</span>  <span class=mf>0.00188</span> <span class=o>+/-</span> <span class=mf>0.00094</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>worst</span> <span class=n>concavity</span>  <span class=mf>0.00164</span> <span class=o>+/-</span> <span class=mf>0.00108</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>mean</span> <span class=n>radius</span>  <span class=mf>0.00141</span> <span class=o>+/-</span> <span class=mf>0.00115</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>compactness</span> <span class=n>error</span>  <span class=mf>0.00141</span> <span class=o>+/-</span> <span class=mf>0.00115</span>
</code></pre></div> <p>In this case, even the most permutation important features have mean importance scores <span class=arithmatex>\(&lt;0.007\)</span>, which doesn't indicate much importance. This is surprising, because we saw via RFE that a linear SVM can achieve <span class=arithmatex>\(\approx 88\%\)</span> classification accuracy with this feature alone. This indicates that worst concave points, in addition to other meaningful features, may belong to subclusters of correlated features. In the corresponding <a href=https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py>scikit-learn example</a>, the authors show that subsets of correlated features can be extracted by calculating a dendogram and selecting representative features from each correlated subset. They achieve <span class=arithmatex>\(97\%\)</span> accuracy (the same as with the full dataset) by selecting only five such representative variables. </p> <h2 id=feature-importance-in-decision-trees>Feature Importance in Decision Trees<a class=headerlink href=#feature-importance-in-decision-trees title="Permanent link">&para;</a></h2> <p>Here we focus on decision trees, which are particularly interpretable classifiers that often appear as ensembles (or <em>boosted decision tree (BDT)</em> algorithms) in HEP. Consider a classification dataset <span class=arithmatex>\(X=\{x_n\}_{n=1}^{N}\)</span>, <span class=arithmatex>\(x_n\in\mathbb{R}^{D}\)</span>, with truth labels <span class=arithmatex>\(Y=\{y_n\}_{n=1}^N\)</span>, <span class=arithmatex>\(y_n\in\{1,...,C\}\)</span> corresponding <span class=arithmatex>\(C\)</span> classes. These truth labels naturally partition <span class=arithmatex>\(X\)</span> into subsets <span class=arithmatex>\(X_c\)</span> with class probabilities <span class=arithmatex>\(p(c)=|X_c|/|X|\)</span>. Decision trees begin with a root node <span class=arithmatex>\(t_0\)</span> containing all of <span class=arithmatex>\(X\)</span>. The tree is grown from the root by recursively splitting the input set <span class=arithmatex>\(X\)</span> in a principled way; internal nodes (or branch nodes) correspond to a decision of the form </p> <div class=arithmatex>\[\begin{aligned} &amp;(x_n)_d\leq\delta \implies\ \text{sample}\ n\ \text{goes to left child node}\\ &amp;(x_n)_d&gt;\delta \implies\ \text{sample}\ n\ \text{goes to right child node} \end{aligned}\]</div> <p>We emphasize that the decision boundary is drawn by considering a single feature field <span class=arithmatex>\(d\)</span> and partitioning the <span class=arithmatex>\(n^\mathrm{th}\)</span> sample by the value at that feature field. Decision boundaries at each internal parent node <span class=arithmatex>\(t_P\)</span> are formed by choosing a "split criterion," which describes how to partition the set of elements at this node into left and right child nodes <span class=arithmatex>\(t_L\)</span>, <span class=arithmatex>\(t_R\)</span> with <span class=arithmatex>\(X_{t_L}\subset X_{t_P}\)</span> and <span class=arithmatex>\(X_{t_R}\subset X_{t_P}\)</span>, <span class=arithmatex>\(X_{t_L}\cup X_{t_R}=X_{t_P}\)</span>. This partitioning is optimal if <span class=arithmatex>\(X_{t_L}\)</span> and <span class=arithmatex>\(X_{t_R}\)</span> are pure, each containing only members of the same class. <em>Impurity measures</em> are used to evaluate the degree to which the set of data points at a given tree node <span class=arithmatex>\(t\)</span> are not pure. One common impurity measure is Gini Impurity, </p> <div class=arithmatex>\[\begin{aligned} I(t) = \sum_{c=1}^C p(c|t)(1-p(c|t)) \end{aligned}\]</div> <p>Here, <span class=arithmatex>\(p(c|t)\)</span> is the probability of drawing a member of class <span class=arithmatex>\(c\)</span> from the set of elements at node <span class=arithmatex>\(t\)</span>. For example, the Gini impurity at the root node (corresponding to the whole dataset) is </p> <div class=arithmatex>\[\begin{aligned} I(t_0) = \sum_{c=1}^C \frac{|X_c|}{|X|}(1-\frac{|X_c|}{|X|}) \end{aligned}\]</div> <p>In a balanced binary dataset, this would give <span class=arithmatex>\(I(t_0)=1/2\)</span>. If the set at node <span class=arithmatex>\(t\)</span> is pure, i.e. class labels corresponding to <span class=arithmatex>\(X_t\)</span> are identical, then <span class=arithmatex>\(I(t)=0\)</span>. We can use <span class=arithmatex>\(I(t)\)</span> to produce an optimal splitting from parent <span class=arithmatex>\(t_p\)</span> to children <span class=arithmatex>\(t_L\)</span> and <span class=arithmatex>\(t_R\)</span> by defining an <em>impurity gain</em>, </p> <div class=arithmatex>\[\begin{aligned} \Delta I = I(t_P) - I(t_L) - I(t_R) \end{aligned}\]</div> <p>This quantity describes the relative impurity between a parent node and its children. If <span class=arithmatex>\(X_{t_P}\)</span> contains only two classes, an optimal splitting would separate them into <span class=arithmatex>\(X_{p_L}\)</span> and <span class=arithmatex>\(X_{p_R}\)</span>, producing pure children nodes with <span class=arithmatex>\(I(t_L)=I(t_R)=0\)</span> and, correspondingly, <span class=arithmatex>\(\Delta I(t_p) = I(t_P)\)</span>. Accordingly, good splitting decisions should maximize impurity gain. Note that the impurity gain is often weighted, for example Scikit-Learn defines:</p> <div class=arithmatex>\[\begin{aligned} \Delta I(t_p) = \frac{|X_{t_p}|}{|X|}\bigg(I(t_p) - \frac{|X_{t_L}|}{|X_{t_p}|} I(t_L) - \frac{|X_{t_R}|}{|X_{t_p}|} I(t_R) \bigg) \end{aligned}\]</div> <p>In general, a pure node cannot be split further and must therefore be a leaf. Likewise, a node for which there is no splitting yielding <span class=arithmatex>\(\Delta I &gt; 0\)</span> must be labeled a leaf. These splitting decisions are made recursively at each node in a tree until some stopping condition is met. Stopping conditions may include maximum tree depths or leaf node counts, or threshhold on the maximum impurity gain. </p> <p>Impurity gain gives us insight into the importance of a decision. In particular, larger <span class=arithmatex>\(\Delta I\)</span> indicates a more important decision. If some feature <span class=arithmatex>\((x_n)_d\)</span> is the basis for several decision splits in a decision tree, the sum of impurity gains at these splits gives insight into the importance of this feature. Accordingly, one measure of the feature importance of <span class=arithmatex>\(d\)</span> is the average (with respect to the total number of internal nodes) impurity gain imparted by decision split on <span class=arithmatex>\(d\)</span>. This method generalizes to the case of BDTs, in which case one would average this quantity across all weak learner trees in the ensemble. </p> <p>Note that though decision trees are based on the feature <span class=arithmatex>\(d\)</span> producing the best (maximum impurity gain) split at a given branch node, <em>surrogate splits</em> are often used to retain additional splits corresponding to features other than <span class=arithmatex>\(d\)</span>. Denote the feature maximizing the impurity gain <span class=arithmatex>\(d_1\)</span> and producing a split boundary <span class=arithmatex>\(\delta_1\)</span>. Surrogte splitting involves tracking secondary splits with boundaries <span class=arithmatex>\(\delta_2, \delta_3,...\)</span> corresponding to <span class=arithmatex>\(d_2,d_3,...\)</span> that have the highest correlation with the maximum impurity gain split. The upshot is that in the event that input data is missing a value at field <span class=arithmatex>\(d_1\)</span>, there are backup decision boundaries to use, mitigating the need to define multiple trees for similar data. Using this generalized notion of a decision tree, wherein each branch node contains a primary decision boundary maximizing impurity gain and several additional surrogate split boundaries, we can average the impurity gain produced at feature field <span class=arithmatex>\(d\)</span> over all its occurances as a decision split or a surrogate split. This definition of feature importance generalizes the previous to include additional correlations. </p> <h3 id=example>Example<a class=headerlink href=#example title="Permanent link">&para;</a></h3> <p>Let us now turn to an example: <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>from</span> <span class=nn>sklearn.tree</span> <span class=kn>import</span> <span class=n>DecisionTreeClassifier</span>
<span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_wine</span>
<span class=kn>from</span> <span class=nn>sklearn.inspection</span> <span class=kn>import</span> <span class=n>DecisionBoundaryDisplay</span>
<span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>log_loss</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>

<span class=n>wine_data</span> <span class=o>=</span> <span class=n>load_wine</span><span class=p>()</span> 
<span class=nb>print</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>target</span><span class=p>))</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>178</span><span class=p>,</span> <span class=mi>13</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>[</span><span class=s1>&#39;alcohol&#39;</span><span class=p>,</span> <span class=s1>&#39;malic_acid&#39;</span><span class=p>,</span> <span class=s1>&#39;ash&#39;</span><span class=p>,</span> <span class=s1>&#39;alcalinity_of_ash&#39;</span><span class=p>,</span> <span class=s1>&#39;magnesium&#39;</span><span class=p>,</span> <span class=s1>&#39;total_phenols&#39;</span><span class=p>,</span> <span class=s1>&#39;flavanoids&#39;</span><span class=p>,</span> <span class=s1>&#39;nonflavanoid_phenols&#39;</span><span class=p>,</span> <span class=s1>&#39;proanthocyanins&#39;</span><span class=p>,</span> <span class=s1>&#39;color_intensity&#39;</span><span class=p>,</span> <span class=s1>&#39;hue&#39;</span><span class=p>,</span> <span class=s1>&#39;od280/od315_of_diluted_wines&#39;</span><span class=p>,</span> <span class=s1>&#39;proline&#39;</span><span class=p>]</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>[</span><span class=mi>0</span> <span class=mi>1</span> <span class=mi>2</span><span class=p>]</span>
</code></pre></div></p> <p>This sklearn wine dataset has 178 entries with 13 features and truth labels corresponding to membership in one of <span class=arithmatex>\(C=3\)</span> classes. We can train a decision tree classifier as follows: </p> <div class=highlight><pre><span></span><code><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>wine_data</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>wine_data</span><span class=o>.</span><span class=n>target</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.25</span><span class=p>)</span>
<span class=n>classifier</span> <span class=o>=</span> <span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>criterion</span><span class=o>=</span><span class=s1>&#39;gini&#39;</span><span class=p>,</span> <span class=n>splitter</span><span class=o>=</span><span class=s1>&#39;best&#39;</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>27</span><span class=p>)</span>
<span class=n>classifier</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>X_test_pred</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Test Set Performance&#39;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Number misclassified:&#39;</span><span class=p>,</span> <span class=nb>sum</span><span class=p>(</span><span class=n>X_test_pred</span><span class=o>!=</span><span class=n>y_test</span><span class=p>))</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Accuracy: </span><span class=si>{</span><span class=n>classifier</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Test</span> <span class=n>Set</span> <span class=n>Performance</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Number</span> <span class=n>misclassified</span><span class=p>:</span> <span class=mi>0</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>1.000</span>
</code></pre></div> <p>In this case, the classifier has generalized perfectly, fitting the test set with <span class=arithmatex>\(100\%\)</span> accuracy. Let's take a look into how it makes predictions:</p> <div class=highlight><pre><span></span><code><span class=n>tree</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>tree_</span>
<span class=n>n_nodes</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>node_count</span>
<span class=n>node_features</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>feature</span>
<span class=n>thresholds</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>threshold</span>
<span class=n>children_L</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>children_left</span>
<span class=n>children_R</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>children_right</span>
<span class=n>feature_names</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;The tree has </span><span class=si>{</span><span class=n>n_nodes</span><span class=si>}</span><span class=s1> nodes&#39;</span><span class=p>)</span>
<span class=k>for</span> <span class=n>n</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_nodes</span><span class=p>):</span>
    <span class=k>if</span> <span class=n>children_L</span><span class=p>[</span><span class=n>n</span><span class=p>]</span><span class=o>==</span><span class=n>children_R</span><span class=p>[</span><span class=n>n</span><span class=p>]:</span> <span class=k>continue</span> <span class=c1># leaf node</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Decision split at node </span><span class=si>{</span><span class=n>n</span><span class=si>}</span><span class=s1>:&#39;</span><span class=p>,</span>
          <span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>feature_names</span><span class=p>[</span><span class=n>node_features</span><span class=p>[</span><span class=n>n</span><span class=p>]]</span><span class=si>}</span><span class=s1>(</span><span class=si>{</span><span class=n>node_features</span><span class=p>[</span><span class=n>n</span><span class=p>]</span><span class=si>}</span><span class=s1>) &lt;=&#39;</span><span class=p>,</span>
          <span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>thresholds</span><span class=p>[</span><span class=n>n</span><span class=p>]</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>The</span> <span class=n>tree</span> <span class=n>has</span> <span class=mi>13</span> <span class=n>nodes</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>0</span><span class=p>:</span> <span class=n>color_intensity</span><span class=p>(</span><span class=mi>9</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>3.46</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>2</span><span class=p>:</span> <span class=n>od280</span><span class=o>/</span><span class=n>od315_of_diluted_wines</span><span class=p>(</span><span class=mi>11</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>2.48</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>3</span><span class=p>:</span> <span class=n>flavanoids</span><span class=p>(</span><span class=mi>6</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>1.40</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>5</span><span class=p>:</span> <span class=n>color_intensity</span><span class=p>(</span><span class=mi>9</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>7.18</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>8</span><span class=p>:</span> <span class=n>proline</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>724.50</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>9</span><span class=p>:</span> <span class=n>malic_acid</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>3.33</span>
</code></pre></div> <p>Here we see that several features are used to generate decision boundaries. For example, the dataset is split at the root node by a cut on the <span class=arithmatex>\(\texttt{color_intensity}\)</span> feature. The importance of each feature can be taken to be the average impurity gain it generates across all nodes, so we expect that one (or several) of the five unique features used at the decision splits will be the most important features by this definition. Indeed, we see,</p> <div class=highlight><pre><span></span><code><span class=n>feature_names</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=n>importances</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>feature_importances_</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>importances</span><span class=p>)):</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>}</span><span class=s1>: </span><span class=si>{</span><span class=n>importances</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>Most important features&#39;</span><span class=p>,</span> 
      <span class=n>feature_names</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>importances</span><span class=p>)[</span><span class=o>-</span><span class=mi>3</span><span class=p>:]])</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>alcohol</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>malic_acid</span><span class=p>:</span> <span class=mf>0.021</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>ash</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>alcalinity_of_ash</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>magnesium</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>total_phenols</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>flavanoids</span><span class=p>:</span> <span class=mf>0.028</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>nonflavanoid_phenols</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>proanthocyanins</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>color_intensity</span><span class=p>:</span> <span class=mf>0.363</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>hue</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>od280</span><span class=o>/</span><span class=n>od315_of_diluted_wines</span><span class=p>:</span> <span class=mf>0.424</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>proline</span><span class=p>:</span> <span class=mf>0.165</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Most</span> <span class=n>important</span> <span class=n>features</span> <span class=p>[</span><span class=s1>&#39;proline&#39;</span> <span class=s1>&#39;color_intensity&#39;</span> <span class=s1>&#39;od280/od315_of_diluted_wines&#39;</span><span class=p>]</span>
</code></pre></div> <p>This is an embedded method for generating feature importance - it's cooked right into the decision tree model. Let's verify these results using a wrapper method, permutation importance:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.inspection</span> <span class=kn>import</span> <span class=n>permutation_importance</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Initial classifier score: </span><span class=si>{</span><span class=n>classifier</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=n>r</span> <span class=o>=</span> <span class=n>permutation_importance</span><span class=p>(</span><span class=n>classifier</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=n>n_repeats</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=o>.</span><span class=n>argsort</span><span class=p>()[::</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>&lt;8</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot; </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot; +/- </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_std</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Initial</span> <span class=n>classifier</span> <span class=n>score</span><span class=p>:</span> <span class=mf>1.000</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>color_intensity</span> <span class=mf>0.266</span> <span class=o>+/-</span> <span class=mf>0.040</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>od280</span><span class=o>/</span><span class=n>od315_of_diluted_wines</span> <span class=mf>0.237</span> <span class=o>+/-</span> <span class=mf>0.049</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>proline</span>  <span class=mf>0.210</span> <span class=o>+/-</span> <span class=mf>0.041</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>flavanoids</span> <span class=mf>0.127</span> <span class=o>+/-</span> <span class=mf>0.025</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>malic_acid</span> <span class=mf>0.004</span> <span class=o>+/-</span> <span class=mf>0.008</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>hue</span>      <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>proanthocyanins</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>nonflavanoid_phenols</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>total_phenols</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>magnesium</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>alcalinity_of_ash</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>ash</span>      <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>alcohol</span>  <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
</code></pre></div> <p>The tree's performance is hurt the most if the <span class=arithmatex>\(\texttt{color_intensity}\)</span>, <span class=arithmatex>\(\texttt{od280/od315_of_diluted_wines}\)</span>, or <span class=arithmatex>\(\texttt{proline}\)</span> features are permuted, consistent with the impurity gain measure of feature importance. </p> <hr> <div class=md-source-date> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">August 1, 2022</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=model_optimization.html title="Model optimization" class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> Model optimization </div> </div> </a> <a href=data_augmentation.html title="Data augmentation" class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> Data augmentation </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 CMS Machine Learning Group </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/cms-ml target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg> </a> <a href=https://hub.docker.com/orgs/cmsml/repositories target=_blank rel=noopener title=hub.docker.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M349.9 236.3h-66.1v-59.4h66.1v59.4zm0-204.3h-66.1v60.7h66.1V32zm78.2 144.8H362v59.4h66.1v-59.4zm-156.3-72.1h-66.1v60.1h66.1v-60.1zm78.1 0h-66.1v60.1h66.1v-60.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1l-13.3-8.9zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1v-59.4zm-78.1-72.1h-66.1v60.1h66.1v-60.1z"/></svg> </a> <a href=https://hypernews.cern.ch/HyperNews/CMS/get/machine-learning.html target=_blank rel=noopener title=hypernews.cern.ch class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M256 32C114.6 32 0 125.1 0 240c0 49.6 21.4 95 57 130.7C44.5 421.1 2.7 466 2.2 466.5c-2.2 2.3-2.8 5.7-1.5 8.7S4.8 480 8 480c66.3 0 116-31.8 140.6-51.4 32.7 12.3 69 19.4 107.4 19.4 141.4 0 256-93.1 256-208S397.4 32 256 32zM128 272c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm128 0c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm128 0c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32z"/></svg> </a> <a href=mailto:hn-cms-machine-learning@cern.ch target=_blank rel=noopener title class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../assets/javascripts/vendor.c3dc8c49.min.js></script> <script src=../assets/javascripts/bundle.f9edbbd5.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script> <script>
        app = initialize({
          base: "..",
          features: ["instant"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.8e2cddea.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src=https://unpkg.com/mermaid@8.6/dist/mermaid.min.js></script> <script src=../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>