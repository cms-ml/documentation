<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Documentation of the CMS Machine Learning Group"><meta name=author content="CMS Machine Learning Group"><link rel=canonical href=https://cms-ml.github.io/documentation/optimization/importance.html><link rel=prev href=model_optimization.html><link rel=next href=data_augmentation.html><link rel=icon href=../images/favicon.png><meta name=generator content="mkdocs-1.6.0, mkdocs-material-9.5.23"><title>Feature importance - CMS Machine Learning Documentation</title><link rel=stylesheet href=../assets/stylesheets/main.6543a935.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../termynal.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=orange> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#feature-importance class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../index.html title="CMS Machine Learning Documentation" class="md-header__button md-logo" aria-label="CMS Machine Learning Documentation" data-md-component=logo> <img src=../images/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> CMS Machine Learning Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Feature importance </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=orange aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=orange aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/cms-ml/documentation title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> cms-ml/documentation </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../index.html title="CMS Machine Learning Documentation" class="md-nav__button md-logo" aria-label="CMS Machine Learning Documentation" data-md-component=logo> <img src=../images/logo.png alt=logo> </a> CMS Machine Learning Documentation </label> <div class=md-nav__source> <a href=https://github.com/cms-ml/documentation title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> cms-ml/documentation </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../index.html class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-nav__item> <a href=../newsletter/newsletters.html class=md-nav__link> <span class=md-ellipsis> Newsletters </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> Innovation </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Innovation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../innovation/journal_club.html class=md-nav__link> <span class=md-ellipsis> ML Journal Club </span> </a> </li> <li class=md-nav__item> <a href=../innovation/hackathons.html class=md-nav__link> <span class=md-ellipsis> ML Hackathons </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../resources/cloud_resources/index.html class=md-nav__link> <span class=md-ellipsis> Cloud Resources </span> </a> </li> <li class=md-nav__item> <a href=../resources/dataset_resources/index.html class=md-nav__link> <span class=md-ellipsis> Dataset Resources </span> </a> </li> <li class=md-nav__item> <a href=../resources/fpga_resources/index.html class=md-nav__link> <span class=md-ellipsis> FPGA Resource </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex=0> <span class=md-ellipsis> GPU Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> GPU Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../resources/gpu_resources/cms_resources/lxplus_gpu.html class=md-nav__link> <span class=md-ellipsis> lxplus-gpu </span> </a> </li> <li class=md-nav__item> <a href=../resources/gpu_resources/cms_resources/lxplus_htcondor.html class=md-nav__link> <span class=md-ellipsis> CERN HTCondor </span> </a> </li> <li class=md-nav__item> <a href=../resources/gpu_resources/cms_resources/swan.html class=md-nav__link> <span class=md-ellipsis> SWAN </span> </a> </li> <li class=md-nav__item> <a href=../resources/gpu_resources/cms_resources/ml_cern_ch.html class=md-nav__link> <span class=md-ellipsis> ml.cern.ch </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> Guides </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Guides </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_1> <label class=md-nav__link for=__nav_5_1 id=__nav_5_1_label tabindex=0> <span class=md-ellipsis> Software environments </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_1_label aria-expanded=false> <label class=md-nav__title for=__nav_5_1> <span class="md-nav__icon md-icon"></span> Software environments </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../software_envs/lcg_environments.html class=md-nav__link> <span class=md-ellipsis> LCG environments </span> </a> </li> <li class=md-nav__item> <a href=../software_envs/containers.html class=md-nav__link> <span class=md-ellipsis> Using containers </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2 checked> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex=0> <span class=md-ellipsis> Optimization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=true> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> Optimization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=model_optimization.html class=md-nav__link> <span class=md-ellipsis> Model optimization </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Feature importance </span> <span class="md-nav__icon md-icon"></span> </label> <a href=importance.html class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Feature importance </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#general-discussion class=md-nav__link> <span class=md-ellipsis> General Discussion </span> </a> <nav class=md-nav aria-label="General Discussion"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#filter-methods class=md-nav__link> <span class=md-ellipsis> Filter Methods </span> </a> </li> <li class=md-nav__item> <a href=#embedded-methods class=md-nav__link> <span class=md-ellipsis> Embedded Methods </span> </a> </li> <li class=md-nav__item> <a href=#wrapper-methods class=md-nav__link> <span class=md-ellipsis> Wrapper Methods </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#introduction-by-example class=md-nav__link> <span class=md-ellipsis> Introduction by Example </span> </a> <nav class=md-nav aria-label="Introduction by Example"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#direct-interpretation class=md-nav__link> <span class=md-ellipsis> Direct Interpretation </span> </a> </li> <li class=md-nav__item> <a href=#permutation-importance class=md-nav__link> <span class=md-ellipsis> Permutation Importance </span> </a> </li> <li class=md-nav__item> <a href=#l1-enforced-sparsity class=md-nav__link> <span class=md-ellipsis> L1-Enforced Sparsity </span> </a> </li> <li class=md-nav__item> <a href=#recursive-feature-elimination class=md-nav__link> <span class=md-ellipsis> Recursive Feature Elimination </span> </a> </li> <li class=md-nav__item> <a href=#feature-correlations class=md-nav__link> <span class=md-ellipsis> Feature Correlations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#feature-importance-in-decision-trees class=md-nav__link> <span class=md-ellipsis> Feature Importance in Decision Trees </span> </a> <nav class=md-nav aria-label="Feature Importance in Decision Trees"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example class=md-nav__link> <span class=md-ellipsis> Example </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=data_augmentation.html class=md-nav__link> <span class=md-ellipsis> Data augmentation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex=0> <span class=md-ellipsis> General Advice </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> General Advice </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../general_advice/intro.html class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3_2> <label class=md-nav__link for=__nav_5_3_2 id=__nav_5_3_2_label tabindex=0> <span class=md-ellipsis> Before training </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3_2> <span class="md-nav__icon md-icon"></span> Before training </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../general_advice/before/domains.html class=md-nav__link> <span class=md-ellipsis> Domains </span> </a> </li> <li class=md-nav__item> <a href=../general_advice/before/features.html class=md-nav__link> <span class=md-ellipsis> Features </span> </a> </li> <li class=md-nav__item> <a href=../general_advice/before/inputs.html class=md-nav__link> <span class=md-ellipsis> Inputs </span> </a> </li> <li class=md-nav__item> <a href=../general_advice/before/model.html class=md-nav__link> <span class=md-ellipsis> Model </span> </a> </li> <li class=md-nav__item> <a href=../general_advice/before/metrics.html class=md-nav__link> <span class=md-ellipsis> Metrics & Losses </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3_3> <label class=md-nav__link for=__nav_5_3_3 id=__nav_5_3_3_label tabindex=0> <span class=md-ellipsis> During training </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3_3> <span class="md-nav__icon md-icon"></span> During training </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../general_advice/during/overfitting.html class=md-nav__link> <span class=md-ellipsis> Overfitting </span> </a> </li> <li class=md-nav__item> <a href=../general_advice/during/xvalidation.html class=md-nav__link> <span class=md-ellipsis> Cross-validation </span> </a> </li> <li class=md-nav__item> <a href=../general_advice/during/opt.html class=md-nav__link> <span class=md-ellipsis> Optimisation problems </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../general_advice/after/after.html class=md-nav__link> <span class=md-ellipsis> After training </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex=0> <span class=md-ellipsis> Inference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> Inference </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4_1> <label class=md-nav__link for=__nav_5_4_1 id=__nav_5_4_1_label tabindex=0> <span class=md-ellipsis> Direct inference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_4_1_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4_1> <span class="md-nav__icon md-icon"></span> Direct inference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/tensorflow2.html class=md-nav__link> <span class=md-ellipsis> TensorFlow 2 </span> </a> </li> <li class=md-nav__item> <a href=../inference/tensorflow_aot.html class=md-nav__link> <span class=md-ellipsis> TensorFlow AOT </span> </a> </li> <li class=md-nav__item> <a href=../inference/pytorch.html class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../inference/pyg.html class=md-nav__link> <span class=md-ellipsis> PyTorch Geometric </span> </a> </li> <li class=md-nav__item> <a href=../inference/onnx.html class=md-nav__link> <span class=md-ellipsis> ONNX </span> </a> </li> <li class=md-nav__item> <a href=../inference/xgboost.html class=md-nav__link> <span class=md-ellipsis> XGBoost </span> </a> </li> <li class=md-nav__item> <a href=../inference/hls4ml.html class=md-nav__link> <span class=md-ellipsis> hls4ml </span> </a> </li> <li class=md-nav__item> <a href=../inference/conifer.html class=md-nav__link> <span class=md-ellipsis> conifer </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4_2> <label class=md-nav__link for=__nav_5_4_2 id=__nav_5_4_2_label tabindex=0> <span class=md-ellipsis> Inference as a service </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4_2> <span class="md-nav__icon md-icon"></span> Inference as a service </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/sonic_triton.html class=md-nav__link> <span class=md-ellipsis> Sonic/Triton </span> </a> </li> <li class=md-nav__item> <a href=../inference/tfaas.html class=md-nav__link> <span class=md-ellipsis> TFaaS </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4_3> <label class=md-nav__link for=__nav_5_4_3 id=__nav_5_4_3_label tabindex=0> <span class=md-ellipsis> Non-standard workflows </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4_3> <span class="md-nav__icon md-icon"></span> Non-standard workflows </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/standalone.html class=md-nav__link> <span class=md-ellipsis> Standalone framework </span> </a> </li> <li class=md-nav__item> <a href=../inference/swan_aws.html class=md-nav__link> <span class=md-ellipsis> SWAN + AWS </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../inference/checklist.html class=md-nav__link> <span class=md-ellipsis> Integration checklist </span> </a> </li> <li class=md-nav__item> <a href=../inference/performance.html class=md-nav__link> <span class=md-ellipsis> Performance </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4_6> <label class=md-nav__link for=__nav_5_4_6 id=__nav_5_4_6_label tabindex=0> <span class=md-ellipsis> Successful integrations </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_4_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4_6> <span class="md-nav__icon md-icon"></span> Successful integrations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../inference/particlenet.html class=md-nav__link> <span class=md-ellipsis> ParticleNet </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex=0> <span class=md-ellipsis> Training </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> Training </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../training/Decorrelation.html class=md-nav__link> <span class=md-ellipsis> Decorrelation </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5_2> <label class=md-nav__link for=__nav_5_5_2 id=__nav_5_5_2_label tabindex=0> <span class=md-ellipsis> Training as a Service </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_5_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5_2> <span class="md-nav__icon md-icon"></span> Training as a Service </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../training/MLaaS4HEP.html class=md-nav__link> <span class=md-ellipsis> MLaaS4HEP </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../training/autoencoders.html class=md-nav__link> <span class=md-ellipsis> Autoencoders </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#general-discussion class=md-nav__link> <span class=md-ellipsis> General Discussion </span> </a> <nav class=md-nav aria-label="General Discussion"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#filter-methods class=md-nav__link> <span class=md-ellipsis> Filter Methods </span> </a> </li> <li class=md-nav__item> <a href=#embedded-methods class=md-nav__link> <span class=md-ellipsis> Embedded Methods </span> </a> </li> <li class=md-nav__item> <a href=#wrapper-methods class=md-nav__link> <span class=md-ellipsis> Wrapper Methods </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#introduction-by-example class=md-nav__link> <span class=md-ellipsis> Introduction by Example </span> </a> <nav class=md-nav aria-label="Introduction by Example"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#direct-interpretation class=md-nav__link> <span class=md-ellipsis> Direct Interpretation </span> </a> </li> <li class=md-nav__item> <a href=#permutation-importance class=md-nav__link> <span class=md-ellipsis> Permutation Importance </span> </a> </li> <li class=md-nav__item> <a href=#l1-enforced-sparsity class=md-nav__link> <span class=md-ellipsis> L1-Enforced Sparsity </span> </a> </li> <li class=md-nav__item> <a href=#recursive-feature-elimination class=md-nav__link> <span class=md-ellipsis> Recursive Feature Elimination </span> </a> </li> <li class=md-nav__item> <a href=#feature-correlations class=md-nav__link> <span class=md-ellipsis> Feature Correlations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#feature-importance-in-decision-trees class=md-nav__link> <span class=md-ellipsis> Feature Importance in Decision Trees </span> </a> <nav class=md-nav aria-label="Feature Importance in Decision Trees"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example class=md-nav__link> <span class=md-ellipsis> Example </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=feature-importance>Feature Importance<a class=headerlink href=#feature-importance title="Permanent link">&para;</a></h1> <p>Feature importance is the impact a specific input field has on a prediction model's output. In general, these impacts can range from no impact (i.e. a feature with no variance) to perfect correlation with the ouput. There are several reasons to consider feature importance: </p> <ul> <li>Important features can be used to create simplified models, e.g. to mitigate overfitting.</li> <li>Using only important features can reduce the latency and memory requirements of the model. </li> <li>The relative importance of a set of features can yield insight into the nature of an otherwise opaque model (improved interpretability). </li> <li>If a model is sensitive to noise, rejecting irrelevant inputs may improve its performance. </li> </ul> <p>In the following subsections, we detail several strategies for evaluating feature importance. We begin with a general discussion of feature importance at a high level before offering a code-based tutorial on some common techniques. We conclude with additional notes and comments in the last section. </p> <h2 id=general-discussion>General Discussion<a class=headerlink href=#general-discussion title="Permanent link">&para;</a></h2> <p>Most feature importance methods fall into one of three broad categories: filter methods, embedding methods, and wrapper methods. Here we give a brief overview of each category with relevant examples: </p> <h3 id=filter-methods>Filter Methods<a class=headerlink href=#filter-methods title="Permanent link">&para;</a></h3> <p>Filter methods do not rely on a specific model, instead considering features in the context of a given dataset. In this way, they may be considered to be pre-processing steps. In many cases, the goal of feature filtering is to reduce high dimensional data. However, these methods are also applicable to data exploration, wherein an analyst simply seeks to learn about a dataset without actually removing any features. This knowledge may help interpret the performance of a downstream predictive model. Relevant examples include, </p> <ul> <li> <p><strong>Domain Knowledge</strong>: Perhaps the most obvious strategy is to select features relevant to the domain of interest. </p> </li> <li> <p><strong>Variance Thresholding</strong>: One basic filtering strategy is to simply remove features with low variance. In the extreme case, features with zero variance do not vary from example to example, and will therefore have no impact on the model's final prediction. Likewise, features with variance below a given threshold may not affect a model's downstream performance.</p> </li> <li> <p><strong>Fisher Scoring</strong>: Fisher scoring can be used to rank features; the analyst would then select the highest scoring features as inputs to a subsequent model. </p> </li> <li> <p><strong>Correlations</strong>: Correlated features introduce a certain degree of redundancy to a dataset, so reducing the number of strongly correlated variables may not impact a model's downstream performance. </p> </li> </ul> <h3 id=embedded-methods>Embedded Methods<a class=headerlink href=#embedded-methods title="Permanent link">&para;</a></h3> <p>Embedded methods are specific to a prediction model and independent of the dataset. Examples:</p> <ul> <li><strong>L1 Regularization (LASSO)</strong>: L1 regularization directly penalizes large model weights. In the context of linear regression, for example, this amounts to enforcing sparsity in the output prediction; weights corresponding to less relevant features will be driven to 0, nullifying the feature's effect on the output. </li> </ul> <h3 id=wrapper-methods>Wrapper Methods<a class=headerlink href=#wrapper-methods title="Permanent link">&para;</a></h3> <p>Wrapper methods iterate on prediction models in the context of a given dataset. In general they may be computationally expensive when compared to filter methods. Examples:</p> <ul> <li><strong>Permutation Importance</strong>: Direct interpretation isn't always feasible, so other methods have been developed to inspect a feature's importance. One common and broadly-applicable method is to randomly shuffle a given feature's input values and test the degredation of model performance. This process allows us to measure <a href=https://scikit-learn.org/stable/modules/permutation_importance.html>permutation importance</a> as follows. First, fit a model (<span class=arithmatex>\(f\)</span>) to training data, yielding <span class=arithmatex>\(f(X_\mathrm{train})\)</span>, where <span class=arithmatex>\(X_\mathrm{train}\in\mathbb{R}^{n\times d}\)</span> for <span class=arithmatex>\(n\)</span> input examples with <span class=arithmatex>\(d\)</span> features. Next, measure the model's performance on testing data for some loss <span class=arithmatex>\(\mathcal{L}\)</span>, i.e. <span class=arithmatex>\(s=\mathcal{L}\big(f(X_\mathrm{test}), y_\mathrm{test}\big)\)</span>. For each feature <span class=arithmatex>\(j\in[1\ ..\ d]\)</span>, randomly shuffle the corresponding column in <span class=arithmatex>\(X_\mathrm{test}\)</span> to form <span class=arithmatex>\(X_\mathrm{test}^{(j)}\)</span>. Repeat this process <span class=arithmatex>\(K\)</span> times, so that for <span class=arithmatex>\(k\in [1\ ..\ K]\)</span> each random shuffling of feature column <span class=arithmatex>\(j\)</span> gives a corrupted input dataset <span class=arithmatex>\(X_\mathrm{test}^{(j,k)}\)</span>. Finally, define the permutation importance of feature <span class=arithmatex>\(j\)</span> as the difference between the un-corrupted validation score and average validation score over the corrupted <span class=arithmatex>\(X_\mathrm{test}^{(j,k)}\)</span> datasets: </li> </ul> <div class=arithmatex>\[\texttt{PI}_j = s - \frac{1}{K}\sum_{k=1}^{K} \mathcal{L}[f(X_\mathrm{test}^{(j,k)}), y_\mathrm{test}]\]</div> <ul> <li><strong>Recursive Feature Elimination (RFE)</strong>: Given a prediction model and test/train dataset splits with <span class=arithmatex>\(D\)</span> initial features, RFE returns the set of <span class=arithmatex>\(d &lt; D\)</span> features that maximize model performance. First, the model is trained on the full set of features. The importance of each feature is ranked depending on the model type (e.g. for regression, the slopes are a sufficient ranking measure; permutation importance may also be used). The least important feature is rejected and the model is retrained. This process is repeated until the most significant <span class=arithmatex>\(d\)</span> features remain. </li> </ul> <h2 id=introduction-by-example>Introduction by Example<a class=headerlink href=#introduction-by-example title="Permanent link">&para;</a></h2> <h3 id=direct-interpretation>Direct Interpretation<a class=headerlink href=#direct-interpretation title="Permanent link">&para;</a></h3> <p>Linear regression is particularly interpretable because the prediction coefficients themselves can be interpreted as a measure of feature importance. Here we will compare this direct interpretation to several model inspection techniques. In the following examples we use the <a href=https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html>Diabetes Dataset</a> available as a <a href=https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset>Scikit-learn toy dataset</a>. This dataset maps 10 biological markers to a 1-dimensional quantitative measure of diabetes progression: </p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_diabetes</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>

<span class=n>diabetes</span> <span class=o>=</span> <span class=n>load_diabetes</span><span class=p>()</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_val</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_val</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>diabetes</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>diabetes</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>331</span><span class=p>,</span><span class=mi>10</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>y_train</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>331</span><span class=p>,)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>X_val</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>111</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>y_val</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>111</span><span class=p>,)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>diabetes</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>,</span> <span class=s1>&#39;sex&#39;</span><span class=p>,</span> <span class=s1>&#39;bmi&#39;</span><span class=p>,</span> <span class=s1>&#39;bp&#39;</span><span class=p>,</span> <span class=s1>&#39;s1&#39;</span><span class=p>,</span> <span class=s1>&#39;s2&#39;</span><span class=p>,</span> <span class=s1>&#39;s3&#39;</span><span class=p>,</span> <span class=s1>&#39;s4&#39;</span><span class=p>,</span> <span class=s1>&#39;s5&#39;</span><span class=p>,</span> <span class=s1>&#39;s6&#39;</span><span class=p>]</span>
</code></pre></div> To begin, let's use Ridge Regression (L2-regularized linear regression) to model diabetes progression as a function of the input markers. The absolute value of a regression coefficient (slope) corresponding to a feature can be interpreted the impact of a feature on the final fit:</p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>Ridge</span>
<span class=kn>from</span> <span class=nn>sklearn.feature_selection</span> <span class=kn>import</span> <span class=n>RFE</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1e-2</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Initial model score: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span><span class=w> </span><span class=n>y_val</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=o>-</span><span class=nb>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)):</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>diabetes</span><span class=o>.</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=nb>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=n>i</span><span class=p>]))</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Initial</span> <span class=n>model</span> <span class=n>score</span><span class=p>:</span> <span class=mf>0.357</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bmi</span><span class=p>:</span> <span class=mf>592.253</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s5</span><span class=p>:</span> <span class=mf>580.078</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bp</span><span class=p>:</span> <span class=mf>297.258</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s1</span><span class=p>:</span> <span class=mf>252.425</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>sex</span><span class=p>:</span> <span class=mf>203.436</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s3</span><span class=p>:</span> <span class=mf>145.196</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s4</span><span class=p>:</span> <span class=mf>97.033</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>age</span><span class=p>:</span> <span class=mf>39.103</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s6</span><span class=p>:</span> <span class=mf>32.945</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s2</span><span class=p>:</span> <span class=mf>20.906</span>
</code></pre></div> These results indicate that the bmi and s5 fields have the largest impact on the output of this regression model, while age, s6, and s2 have the smallest. Further interpretation is subject to the nature of the input data (see <a href=https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py>Common Pitfalls in the Interpretation of Coefficients of Linear Models</a>). Note that scikit-learn has <a href=https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel>tools</a> available to faciliate feature selections. </p> <h3 id=permutation-importance>Permutation Importance<a class=headerlink href=#permutation-importance title="Permanent link">&para;</a></h3> <p>In the context of our ridge regression example, we can calculate the permutation importance of each feature as follows (based on <a href=https://scikit-learn.org/stable/modules/permutation_importance.html]>scikit-learn docs</a>):</p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.inspection</span> <span class=kn>import</span> <span class=n>permutation_importance</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1e-2</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Initial model score: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span><span class=w> </span><span class=n>y_val</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=n>r</span> <span class=o>=</span> <span class=n>permutation_importance</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>,</span> <span class=n>n_repeats</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=o>.</span><span class=n>argsort</span><span class=p>()[::</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>diabetes</span><span class=o>.</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>&lt;8</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot; +/- </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_std</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Initial</span> <span class=n>model</span> <span class=n>score</span><span class=p>:</span> <span class=mf>0.357</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s5</span>      <span class=mf>0.204</span> <span class=o>+/-</span> <span class=mf>0.050</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bmi</span>     <span class=mf>0.176</span> <span class=o>+/-</span> <span class=mf>0.048</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bp</span>      <span class=mf>0.088</span> <span class=o>+/-</span> <span class=mf>0.033</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>sex</span>     <span class=mf>0.056</span> <span class=o>+/-</span> <span class=mf>0.023</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s1</span>      <span class=mf>0.042</span> <span class=o>+/-</span> <span class=mf>0.031</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s4</span>      <span class=mf>0.003</span> <span class=o>+/-</span> <span class=mf>0.008</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s6</span>      <span class=mf>0.003</span> <span class=o>+/-</span> <span class=mf>0.003</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s3</span>      <span class=mf>0.002</span> <span class=o>+/-</span> <span class=mf>0.013</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s2</span>      <span class=mf>0.002</span> <span class=o>+/-</span> <span class=mf>0.003</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>age</span>     <span class=o>-</span><span class=mf>0.002</span> <span class=o>+/-</span> <span class=mf>0.004</span>
</code></pre></div> These results are roughly consistent with the direct interpretation of the linear regression parameters; s5 and bmi are the most permutation-important features. This is because both have significant permutation importance scores (0.204, 0.176) when compared to the initial model score (0.357), meaning their random permutations significantly degraded the model perforamnce. On the other hand, s2 and age have approximately no permutation importance, meaning that the model's performance was robust to random permutations of these features. </p> <h3 id=l1-enforced-sparsity>L1-Enforced Sparsity<a class=headerlink href=#l1-enforced-sparsity title="Permanent link">&para;</a></h3> <p>In some applications it may be useful to reject features with low importance. Models biased towards sparsity are one way to achieve this goal, as they are designed to ignore a subset of features with the least impact on the model's output. In the context of linear regression, sparsity can be enforced by imposing L1 regularization on the regression coefficients (LASSO regression):</p> <div class=arithmatex>\[\mathcal{L}_\mathrm{LASSO} = \frac{1}{2n}||y - Xw||^2_2 + \alpha||w||_1\]</div> <p>Depending on the strength of the regularization <span class=arithmatex>\((\alpha)\)</span>, this loss function is biased to zero-out features of low importance. In our diabetes regression example, </p> <p><div class=highlight><pre><span></span><code><span class=n>model</span> <span class=o>=</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1e-1</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Model score: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span><span class=w> </span><span class=n>y_val</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=o>-</span><span class=nb>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)):</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>diabetes</span><span class=o>.</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>}</span><span class=s1>: </span><span class=si>{</span><span class=nb>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=n>i</span><span class=p>])</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Model</span> <span class=n>score</span><span class=p>:</span> <span class=mf>0.355</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bmi</span><span class=p>:</span> <span class=mf>592.203</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s5</span><span class=p>:</span> <span class=mf>507.363</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>bp</span><span class=p>:</span> <span class=mf>240.124</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s3</span><span class=p>:</span> <span class=mf>219.104</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>sex</span><span class=p>:</span> <span class=mf>129.784</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s2</span><span class=p>:</span> <span class=mf>47.628</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s1</span><span class=p>:</span> <span class=mf>41.641</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>age</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s4</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>s6</span><span class=p>:</span> <span class=mf>0.000</span>
</code></pre></div> For this value of <span class=arithmatex>\(\alpha\)</span>, we see that the model has rejected the age, s4, and s6 features as unimportant (consistent with the permutation importance measures above) while achieving a similar model score as the previous ridge regression strategy. </p> <h3 id=recursive-feature-elimination>Recursive Feature Elimination<a class=headerlink href=#recursive-feature-elimination title="Permanent link">&para;</a></h3> <p>Another common strategy is recursive feature elimination (RFE). Though RFE can be used for regression applications as well, we turn our attention to a classification task for the sake of variety. The following discussions are based on the <a href=https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+Diagnostic>Breast Cancer Wisconsin Diagnostic Dataset</a>, which maps 30 numeric features corresponding to digitized breast mass images to a binary classification of benign or malignant. </p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_breast_cancer</span>
<span class=kn>from</span> <span class=nn>sklearn.svm</span> <span class=kn>import</span> <span class=n>SVC</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>StratifiedKFold</span>

<span class=n>data</span> <span class=o>=</span> <span class=n>load_breast_cancer</span><span class=p>()</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_val</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_val</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>data</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>data</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>426</span><span class=p>,</span> <span class=mi>30</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>y_train</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>426</span><span class=p>,)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>X_val</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>143</span><span class=p>,</span> <span class=mi>30</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>y_val</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>143</span><span class=p>,)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>breast_cancer</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>[</span><span class=s1>&#39;mean radius&#39;</span> <span class=s1>&#39;mean texture&#39;</span> <span class=s1>&#39;mean perimeter&#39;</span> <span class=s1>&#39;mean area&#39;</span> <span class=s1>&#39;mean smoothness&#39;</span> <span class=s1>&#39;mean compactness&#39;</span> <span class=s1>&#39;mean concavity&#39;</span> <span class=s1>&#39;mean concave points&#39;</span> <span class=s1>&#39;mean symmetry&#39;</span> <span class=s1>&#39;mean fractal dimension&#39;</span> <span class=s1>&#39;radius error&#39;</span> <span class=s1>&#39;texture error&#39;</span> <span class=s1>&#39;perimeter error&#39;</span> <span class=s1>&#39;area error&#39;</span> <span class=s1>&#39;smoothness error&#39;</span> <span class=s1>&#39;compactness error&#39;</span> <span class=s1>&#39;concavity error&#39;</span> <span class=s1>&#39;concave points error&#39;</span> <span class=s1>&#39;symmetry error&#39;</span> <span class=s1>&#39;fractal dimension error&#39;</span> <span class=s1>&#39;worst radius&#39;</span> <span class=s1>&#39;worst texture&#39;</span> <span class=s1>&#39;worst perimeter&#39;</span> <span class=s1>&#39;worst area&#39;</span> <span class=s1>&#39;worst smoothness&#39;</span> <span class=s1>&#39;worst compactness&#39;</span> <span class=s1>&#39;worst concavity&#39;</span> <span class=s1>&#39;worst concave points&#39;</span> <span class=s1>&#39;worst symmetry&#39;</span> <span class=s1>&#39;worst fractal dimension&#39;</span><span class=p>]</span>
</code></pre></div> <p>Given a classifier and a classification task, recursive feature elimination (RFE, <a href=https://link.springer.com/content/pdf/10.1023/A:1012487302797.pdf>see original paper</a>) is the process of identifying the subset of input features leading to the most performative model. Here we employ a support vector machine classifier (SVM) with a linear kernel to perform binary classification on the input data. We ask for the top <span class=arithmatex>\(j\in[1\ .. \ d]\)</span> most important features in a for loop, computing the classification accuracy when only these features are leveraged. </p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.feature_selection</span> <span class=kn>import</span> <span class=n>RFE</span>

<span class=n>features</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>breast_cancer</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=n>svc</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;linear&#39;</span><span class=p>)</span>
<span class=k>for</span> <span class=n>n_features</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>30</span><span class=p>,</span> <span class=mi>1</span><span class=p>):</span>
    <span class=n>rfe</span> <span class=o>=</span> <span class=n>RFE</span><span class=p>(</span><span class=n>estimator</span><span class=o>=</span><span class=n>svc</span><span class=p>,</span> <span class=n>step</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_features_to_select</span><span class=o>=</span><span class=n>n_features</span><span class=p>)</span>
    <span class=n>rfe</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;n_features=</span><span class=si>{</span><span class=n>n_features</span><span class=si>}</span><span class=s1>, accuracy=</span><span class=si>{</span><span class=n>rfe</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span><span class=w> </span><span class=n>y_val</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39; - selected: </span><span class=si>{</span><span class=n>features</span><span class=p>[</span><span class=n>rfe</span><span class=o>.</span><span class=n>support_</span><span class=p>]</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.881</span>
<span class=o>&gt;&gt;&gt;</span>  <span class=o>-</span> <span class=n>selected</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;worst concave points&#39;</span><span class=p>]</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.874</span>
<span class=o>&gt;&gt;&gt;</span>  <span class=o>-</span> <span class=n>selected</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;worst concavity&#39;</span> <span class=s1>&#39;worst concave points&#39;</span><span class=p>]</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.867</span>
<span class=o>&gt;&gt;&gt;</span>  <span class=o>-</span> <span class=n>selected</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;mean concave points&#39;</span> <span class=s1>&#39;worst concavity&#39;</span> <span class=s1>&#39;worst concave points&#39;</span><span class=p>]</span>
 <span class=o>...</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.930</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>17</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.965</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>18</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.951</span>
<span class=o>...</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>27</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.958</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>28</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.958</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>29</span><span class=p>,</span> <span class=n>accuracy</span><span class=o>=</span><span class=mf>0.958</span>
</code></pre></div> Here we've shown a subset of the output. In the first output lines, we see that the 'worst concave points' feature alone leads to 88.1% accuracy. Including the next two most important features actually degrades the classification accuracy. We then skip to the top 17 features, which in this case we observe to yield the best performance for the linear SVM classifier. The addition of more features does not lead to additional perforamnce boosts. In this way, RFE can be treated as a model wrapper introducing an additional hyperparameter, n_features_to_select, which can be used to optimize model performance. A more principled optimization using k-fold cross validation with RFE is available in the <a href=https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py>scikit-learn docs</a>. </p> <h3 id=feature-correlations>Feature Correlations<a class=headerlink href=#feature-correlations title="Permanent link">&para;</a></h3> <p>In the above, we have focused specifically on interpreting the importance of single features. However, it may be that several features are correlated, sharing the responsibility for the overall prediction of the model. In this case, some measures of feature importance may inappropriately downweight correlated features in a so-called correlation bias (see <a href=https://pubmed.ncbi.nlm.nih.gov/21576180/ >Classification with Correlated Features: Unrelability of Feature Ranking and Solutions</a>). For example, the permutation invariance of <span class=arithmatex>\(d\)</span> correlated features is shown to decrease (as a function of correlation strength) faster for higher <span class=arithmatex>\(d\)</span> (see <a href=https://link.springer.com/article/10.1007/s11222-016-9646-1>Correlation and Variable importance in Random Forests</a>). </p> <p>We can see these effects in action using the breast cancer dataset, following the corresponding <a href=https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py>scikit-learn example</a></p> <p><div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.ensemble</span> <span class=kn>import</span> <span class=n>RandomForestClassifier</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
<span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_breast_cancer</span>

<span class=n>data</span> <span class=o>=</span> <span class=n>load_breast_cancer</span><span class=p>()</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>data</span><span class=o>.</span><span class=n>target</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=n>clf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>clf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Accuracy on test data: </span><span class=si>{:.2f}</span><span class=s2>&quot;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>clf</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)))</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Accuracy</span> <span class=n>on</span> <span class=n>test</span> <span class=n>data</span><span class=p>:</span> <span class=mf>0.97</span>
</code></pre></div> Here we've implemented a random forest classifier and achieved a high accuracy (97%) on the benign vs. malignent predictions. The permutation importances for the 10 most important training features are:</p> <div class=highlight><pre><span></span><code><span class=n>r</span> <span class=o>=</span> <span class=n>permutation_importance</span><span class=p>(</span><span class=n>clf</span><span class=p>,</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>n_repeats</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=o>.</span><span class=n>argsort</span><span class=p>()[::</span><span class=o>-</span><span class=mi>1</span><span class=p>][:</span><span class=mi>10</span><span class=p>]:</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>breast_cancer</span><span class=o>.</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>&lt;8</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot;  </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.5f</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot; +/- </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_std</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.5f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>worst</span> <span class=n>concave</span> <span class=n>points</span>  <span class=mf>0.00681</span> <span class=o>+/-</span> <span class=mf>0.00305</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>mean</span> <span class=n>concave</span> <span class=n>points</span>  <span class=mf>0.00329</span> <span class=o>+/-</span> <span class=mf>0.00188</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>worst</span> <span class=n>texture</span>  <span class=mf>0.00258</span> <span class=o>+/-</span> <span class=mf>0.00070</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>radius</span> <span class=n>error</span>  <span class=mf>0.00235</span> <span class=o>+/-</span> <span class=mf>0.00000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>mean</span> <span class=n>texture</span>  <span class=mf>0.00188</span> <span class=o>+/-</span> <span class=mf>0.00094</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>mean</span> <span class=n>compactness</span>  <span class=mf>0.00188</span> <span class=o>+/-</span> <span class=mf>0.00094</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>area</span> <span class=n>error</span>  <span class=mf>0.00188</span> <span class=o>+/-</span> <span class=mf>0.00094</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>worst</span> <span class=n>concavity</span>  <span class=mf>0.00164</span> <span class=o>+/-</span> <span class=mf>0.00108</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>mean</span> <span class=n>radius</span>  <span class=mf>0.00141</span> <span class=o>+/-</span> <span class=mf>0.00115</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>compactness</span> <span class=n>error</span>  <span class=mf>0.00141</span> <span class=o>+/-</span> <span class=mf>0.00115</span>
</code></pre></div> <p>In this case, even the most permutation important features have mean importance scores <span class=arithmatex>\(&lt;0.007\)</span>, which doesn't indicate much importance. This is surprising, because we saw via RFE that a linear SVM can achieve <span class=arithmatex>\(\approx 88\%\)</span> classification accuracy with this feature alone. This indicates that worst concave points, in addition to other meaningful features, may belong to subclusters of correlated features. In the corresponding <a href=https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py>scikit-learn example</a>, the authors show that subsets of correlated features can be extracted by calculating a dendogram and selecting representative features from each correlated subset. They achieve <span class=arithmatex>\(97\%\)</span> accuracy (the same as with the full dataset) by selecting only five such representative variables. </p> <h2 id=feature-importance-in-decision-trees>Feature Importance in Decision Trees<a class=headerlink href=#feature-importance-in-decision-trees title="Permanent link">&para;</a></h2> <p>Here we focus on decision trees, which are particularly interpretable classifiers that often appear as ensembles (or <em>boosted decision tree (BDT)</em> algorithms) in HEP. Consider a classification dataset <span class=arithmatex>\(X=\{x_n\}_{n=1}^{N}\)</span>, <span class=arithmatex>\(x_n\in\mathbb{R}^{D}\)</span>, with truth labels <span class=arithmatex>\(Y=\{y_n\}_{n=1}^N\)</span>, <span class=arithmatex>\(y_n\in\{1,...,C\}\)</span> corresponding <span class=arithmatex>\(C\)</span> classes. These truth labels naturally partition <span class=arithmatex>\(X\)</span> into subsets <span class=arithmatex>\(X_c\)</span> with class probabilities <span class=arithmatex>\(p(c)=|X_c|/|X|\)</span>. Decision trees begin with a root node <span class=arithmatex>\(t_0\)</span> containing all of <span class=arithmatex>\(X\)</span>. The tree is grown from the root by recursively splitting the input set <span class=arithmatex>\(X\)</span> in a principled way; internal nodes (or branch nodes) correspond to a decision of the form </p> <div class=arithmatex>\[\begin{aligned} &amp;(x_n)_d\leq\delta \implies\ \text{sample}\ n\ \text{goes to left child node}\\ &amp;(x_n)_d&gt;\delta \implies\ \text{sample}\ n\ \text{goes to right child node} \end{aligned}\]</div> <p>We emphasize that the decision boundary is drawn by considering a single feature field <span class=arithmatex>\(d\)</span> and partitioning the <span class=arithmatex>\(n^\mathrm{th}\)</span> sample by the value at that feature field. Decision boundaries at each internal parent node <span class=arithmatex>\(t_P\)</span> are formed by choosing a "split criterion," which describes how to partition the set of elements at this node into left and right child nodes <span class=arithmatex>\(t_L\)</span>, <span class=arithmatex>\(t_R\)</span> with <span class=arithmatex>\(X_{t_L}\subset X_{t_P}\)</span> and <span class=arithmatex>\(X_{t_R}\subset X_{t_P}\)</span>, <span class=arithmatex>\(X_{t_L}\cup X_{t_R}=X_{t_P}\)</span>. This partitioning is optimal if <span class=arithmatex>\(X_{t_L}\)</span> and <span class=arithmatex>\(X_{t_R}\)</span> are pure, each containing only members of the same class. <em>Impurity measures</em> are used to evaluate the degree to which the set of data points at a given tree node <span class=arithmatex>\(t\)</span> are not pure. One common impurity measure is Gini Impurity, </p> <div class=arithmatex>\[\begin{aligned} I(t) = \sum_{c=1}^C p(c|t)(1-p(c|t)) \end{aligned}\]</div> <p>Here, <span class=arithmatex>\(p(c|t)\)</span> is the probability of drawing a member of class <span class=arithmatex>\(c\)</span> from the set of elements at node <span class=arithmatex>\(t\)</span>. For example, the Gini impurity at the root node (corresponding to the whole dataset) is </p> <div class=arithmatex>\[\begin{aligned} I(t_0) = \sum_{c=1}^C \frac{|X_c|}{|X|}(1-\frac{|X_c|}{|X|}) \end{aligned}\]</div> <p>In a balanced binary dataset, this would give <span class=arithmatex>\(I(t_0)=1/2\)</span>. If the set at node <span class=arithmatex>\(t\)</span> is pure, i.e. class labels corresponding to <span class=arithmatex>\(X_t\)</span> are identical, then <span class=arithmatex>\(I(t)=0\)</span>. We can use <span class=arithmatex>\(I(t)\)</span> to produce an optimal splitting from parent <span class=arithmatex>\(t_p\)</span> to children <span class=arithmatex>\(t_L\)</span> and <span class=arithmatex>\(t_R\)</span> by defining an <em>impurity gain</em>, </p> <div class=arithmatex>\[\begin{aligned} \Delta I = I(t_P) - I(t_L) - I(t_R) \end{aligned}\]</div> <p>This quantity describes the relative impurity between a parent node and its children. If <span class=arithmatex>\(X_{t_P}\)</span> contains only two classes, an optimal splitting would separate them into <span class=arithmatex>\(X_{p_L}\)</span> and <span class=arithmatex>\(X_{p_R}\)</span>, producing pure children nodes with <span class=arithmatex>\(I(t_L)=I(t_R)=0\)</span> and, correspondingly, <span class=arithmatex>\(\Delta I(t_p) = I(t_P)\)</span>. Accordingly, good splitting decisions should maximize impurity gain. Note that the impurity gain is often weighted, for example Scikit-Learn defines:</p> <div class=arithmatex>\[\begin{aligned} \Delta I(t_p) = \frac{|X_{t_p}|}{|X|}\bigg(I(t_p) - \frac{|X_{t_L}|}{|X_{t_p}|} I(t_L) - \frac{|X_{t_R}|}{|X_{t_p}|} I(t_R) \bigg) \end{aligned}\]</div> <p>In general, a pure node cannot be split further and must therefore be a leaf. Likewise, a node for which there is no splitting yielding <span class=arithmatex>\(\Delta I &gt; 0\)</span> must be labeled a leaf. These splitting decisions are made recursively at each node in a tree until some stopping condition is met. Stopping conditions may include maximum tree depths or leaf node counts, or threshhold on the maximum impurity gain. </p> <p>Impurity gain gives us insight into the importance of a decision. In particular, larger <span class=arithmatex>\(\Delta I\)</span> indicates a more important decision. If some feature <span class=arithmatex>\((x_n)_d\)</span> is the basis for several decision splits in a decision tree, the sum of impurity gains at these splits gives insight into the importance of this feature. Accordingly, one measure of the feature importance of <span class=arithmatex>\(d\)</span> is the average (with respect to the total number of internal nodes) impurity gain imparted by decision split on <span class=arithmatex>\(d\)</span>. This method generalizes to the case of BDTs, in which case one would average this quantity across all weak learner trees in the ensemble. </p> <p>Note that though decision trees are based on the feature <span class=arithmatex>\(d\)</span> producing the best (maximum impurity gain) split at a given branch node, <em>surrogate splits</em> are often used to retain additional splits corresponding to features other than <span class=arithmatex>\(d\)</span>. Denote the feature maximizing the impurity gain <span class=arithmatex>\(d_1\)</span> and producing a split boundary <span class=arithmatex>\(\delta_1\)</span>. Surrogte splitting involves tracking secondary splits with boundaries <span class=arithmatex>\(\delta_2, \delta_3,...\)</span> corresponding to <span class=arithmatex>\(d_2,d_3,...\)</span> that have the highest correlation with the maximum impurity gain split. The upshot is that in the event that input data is missing a value at field <span class=arithmatex>\(d_1\)</span>, there are backup decision boundaries to use, mitigating the need to define multiple trees for similar data. Using this generalized notion of a decision tree, wherein each branch node contains a primary decision boundary maximizing impurity gain and several additional surrogate split boundaries, we can average the impurity gain produced at feature field <span class=arithmatex>\(d\)</span> over all its occurances as a decision split or a surrogate split. This definition of feature importance generalizes the previous to include additional correlations. </p> <h3 id=example>Example<a class=headerlink href=#example title="Permanent link">&para;</a></h3> <p>Let us now turn to an example: <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>from</span> <span class=nn>sklearn.tree</span> <span class=kn>import</span> <span class=n>DecisionTreeClassifier</span>
<span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_wine</span>
<span class=kn>from</span> <span class=nn>sklearn.inspection</span> <span class=kn>import</span> <span class=n>DecisionBoundaryDisplay</span>
<span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>log_loss</span>
<span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>

<span class=n>wine_data</span> <span class=o>=</span> <span class=n>load_wine</span><span class=p>()</span> 
<span class=nb>print</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>target</span><span class=p>))</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>(</span><span class=mi>178</span><span class=p>,</span> <span class=mi>13</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>[</span><span class=s1>&#39;alcohol&#39;</span><span class=p>,</span> <span class=s1>&#39;malic_acid&#39;</span><span class=p>,</span> <span class=s1>&#39;ash&#39;</span><span class=p>,</span> <span class=s1>&#39;alcalinity_of_ash&#39;</span><span class=p>,</span> <span class=s1>&#39;magnesium&#39;</span><span class=p>,</span> <span class=s1>&#39;total_phenols&#39;</span><span class=p>,</span> <span class=s1>&#39;flavanoids&#39;</span><span class=p>,</span> <span class=s1>&#39;nonflavanoid_phenols&#39;</span><span class=p>,</span> <span class=s1>&#39;proanthocyanins&#39;</span><span class=p>,</span> <span class=s1>&#39;color_intensity&#39;</span><span class=p>,</span> <span class=s1>&#39;hue&#39;</span><span class=p>,</span> <span class=s1>&#39;od280/od315_of_diluted_wines&#39;</span><span class=p>,</span> <span class=s1>&#39;proline&#39;</span><span class=p>]</span>
<span class=o>&gt;&gt;&gt;</span> <span class=p>[</span><span class=mi>0</span> <span class=mi>1</span> <span class=mi>2</span><span class=p>]</span>
</code></pre></div></p> <p>This sklearn wine dataset has 178 entries with 13 features and truth labels corresponding to membership in one of <span class=arithmatex>\(C=3\)</span> classes. We can train a decision tree classifier as follows: </p> <div class=highlight><pre><span></span><code><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>wine_data</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>wine_data</span><span class=o>.</span><span class=n>target</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.25</span><span class=p>)</span>
<span class=n>classifier</span> <span class=o>=</span> <span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>criterion</span><span class=o>=</span><span class=s1>&#39;gini&#39;</span><span class=p>,</span> <span class=n>splitter</span><span class=o>=</span><span class=s1>&#39;best&#39;</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>27</span><span class=p>)</span>
<span class=n>classifier</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>X_test_pred</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Test Set Performance&#39;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Number misclassified:&#39;</span><span class=p>,</span> <span class=nb>sum</span><span class=p>(</span><span class=n>X_test_pred</span><span class=o>!=</span><span class=n>y_test</span><span class=p>))</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Accuracy: </span><span class=si>{</span><span class=n>classifier</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span><span class=w> </span><span class=n>y_test</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Test</span> <span class=n>Set</span> <span class=n>Performance</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Number</span> <span class=n>misclassified</span><span class=p>:</span> <span class=mi>0</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>1.000</span>
</code></pre></div> <p>In this case, the classifier has generalized perfectly, fitting the test set with <span class=arithmatex>\(100\%\)</span> accuracy. Let's take a look into how it makes predictions:</p> <div class=highlight><pre><span></span><code><span class=n>tree</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>tree_</span>
<span class=n>n_nodes</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>node_count</span>
<span class=n>node_features</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>feature</span>
<span class=n>thresholds</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>threshold</span>
<span class=n>children_L</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>children_left</span>
<span class=n>children_R</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>children_right</span>
<span class=n>feature_names</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;The tree has </span><span class=si>{</span><span class=n>n_nodes</span><span class=si>}</span><span class=s1> nodes&#39;</span><span class=p>)</span>
<span class=k>for</span> <span class=n>n</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_nodes</span><span class=p>):</span>
    <span class=k>if</span> <span class=n>children_L</span><span class=p>[</span><span class=n>n</span><span class=p>]</span><span class=o>==</span><span class=n>children_R</span><span class=p>[</span><span class=n>n</span><span class=p>]:</span> <span class=k>continue</span> <span class=c1># leaf node</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Decision split at node </span><span class=si>{</span><span class=n>n</span><span class=si>}</span><span class=s1>:&#39;</span><span class=p>,</span>
          <span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>feature_names</span><span class=p>[</span><span class=n>node_features</span><span class=p>[</span><span class=n>n</span><span class=p>]]</span><span class=si>}</span><span class=s1>(</span><span class=si>{</span><span class=n>node_features</span><span class=p>[</span><span class=n>n</span><span class=p>]</span><span class=si>}</span><span class=s1>) &lt;=&#39;</span><span class=p>,</span>
          <span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>thresholds</span><span class=p>[</span><span class=n>n</span><span class=p>]</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>The</span> <span class=n>tree</span> <span class=n>has</span> <span class=mi>13</span> <span class=n>nodes</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>0</span><span class=p>:</span> <span class=n>color_intensity</span><span class=p>(</span><span class=mi>9</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>3.46</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>2</span><span class=p>:</span> <span class=n>od280</span><span class=o>/</span><span class=n>od315_of_diluted_wines</span><span class=p>(</span><span class=mi>11</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>2.48</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>3</span><span class=p>:</span> <span class=n>flavanoids</span><span class=p>(</span><span class=mi>6</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>1.40</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>5</span><span class=p>:</span> <span class=n>color_intensity</span><span class=p>(</span><span class=mi>9</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>7.18</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>8</span><span class=p>:</span> <span class=n>proline</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>724.50</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>Decision</span> <span class=n>split</span> <span class=n>at</span> <span class=n>node</span> <span class=mi>9</span><span class=p>:</span> <span class=n>malic_acid</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>3.33</span>
</code></pre></div> <p>Here we see that several features are used to generate decision boundaries. For example, the dataset is split at the root node by a cut on the <span class=arithmatex>\(\texttt{color_intensity}\)</span> feature. The importance of each feature can be taken to be the average impurity gain it generates across all nodes, so we expect that one (or several) of the five unique features used at the decision splits will be the most important features by this definition. Indeed, we see,</p> <div class=highlight><pre><span></span><code><span class=n>feature_names</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>wine_data</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
<span class=n>importances</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>feature_importances_</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>importances</span><span class=p>)):</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>}</span><span class=s1>: </span><span class=si>{</span><span class=n>importances</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>Most important features&#39;</span><span class=p>,</span> 
      <span class=n>feature_names</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>importances</span><span class=p>)[</span><span class=o>-</span><span class=mi>3</span><span class=p>:]])</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>alcohol</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>malic_acid</span><span class=p>:</span> <span class=mf>0.021</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>ash</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>alcalinity_of_ash</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>magnesium</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>total_phenols</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>flavanoids</span><span class=p>:</span> <span class=mf>0.028</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>nonflavanoid_phenols</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>proanthocyanins</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>color_intensity</span><span class=p>:</span> <span class=mf>0.363</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>hue</span><span class=p>:</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>od280</span><span class=o>/</span><span class=n>od315_of_diluted_wines</span><span class=p>:</span> <span class=mf>0.424</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>proline</span><span class=p>:</span> <span class=mf>0.165</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Most</span> <span class=n>important</span> <span class=n>features</span> <span class=p>[</span><span class=s1>&#39;proline&#39;</span> <span class=s1>&#39;color_intensity&#39;</span> <span class=s1>&#39;od280/od315_of_diluted_wines&#39;</span><span class=p>]</span>
</code></pre></div> <p>This is an embedded method for generating feature importance - it's cooked right into the decision tree model. Let's verify these results using a wrapper method, permutation importance:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>sklearn.inspection</span> <span class=kn>import</span> <span class=n>permutation_importance</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Initial classifier score: </span><span class=si>{</span><span class=n>classifier</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span><span class=w> </span><span class=n>y_test</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=n>r</span> <span class=o>=</span> <span class=n>permutation_importance</span><span class=p>(</span><span class=n>classifier</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=n>n_repeats</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=o>.</span><span class=n>argsort</span><span class=p>()[::</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>feature_names</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>&lt;8</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot; </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_mean</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span>
          <span class=sa>f</span><span class=s2>&quot; +/- </span><span class=si>{</span><span class=n>r</span><span class=o>.</span><span class=n>importances_std</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>Initial</span> <span class=n>classifier</span> <span class=n>score</span><span class=p>:</span> <span class=mf>1.000</span>

<span class=o>&gt;&gt;&gt;</span> <span class=n>color_intensity</span> <span class=mf>0.266</span> <span class=o>+/-</span> <span class=mf>0.040</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>od280</span><span class=o>/</span><span class=n>od315_of_diluted_wines</span> <span class=mf>0.237</span> <span class=o>+/-</span> <span class=mf>0.049</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>proline</span>  <span class=mf>0.210</span> <span class=o>+/-</span> <span class=mf>0.041</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>flavanoids</span> <span class=mf>0.127</span> <span class=o>+/-</span> <span class=mf>0.025</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>malic_acid</span> <span class=mf>0.004</span> <span class=o>+/-</span> <span class=mf>0.008</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>hue</span>      <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>proanthocyanins</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>nonflavanoid_phenols</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>total_phenols</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>magnesium</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>alcalinity_of_ash</span> <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>ash</span>      <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
<span class=o>&gt;&gt;&gt;</span> <span class=n>alcohol</span>  <span class=mf>0.000</span> <span class=o>+/-</span> <span class=mf>0.000</span>
</code></pre></div> <p>The tree's performance is hurt the most if the <span class=arithmatex>\(\texttt{color_intensity}\)</span>, <span class=arithmatex>\(\texttt{od280/od315_of_diluted_wines}\)</span>, or <span class=arithmatex>\(\texttt{proline}\)</span> features are permuted, consistent with the impurity gain measure of feature importance. </p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 17, 2024</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2020-2023 CMS Machine Learning Group </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/cms-ml target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 480 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg> </a> <a href=https://hub.docker.com/orgs/cmsml/repositories target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1v59.4zm0-204.3h-66.1v60.7h66.1V32zm78.2 144.8H362v59.4h66.1v-59.4zm-156.3-72.1h-66.1v60.1h66.1v-60.1zm78.1 0h-66.1v60.1h66.1v-60.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1l-13.3-8.9zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1v-59.4zm-78.1-72.1h-66.1v60.1h66.1v-60.1z"/></svg> </a> <a href=https://cms-talk.web.cern.ch/c/physics/ml/104 target=_blank rel=noopener title=cms-talk.web.cern.ch class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 448c141.4 0 256-93.1 256-208S397.4 32 256 32 0 125.1 0 240c0 45.1 17.7 86.8 47.7 120.9-1.9 24.5-11.4 46.3-21.4 62.9-5.5 9.2-11.1 16.6-15.2 21.6-2.1 2.5-3.7 4.4-4.9 5.7-.6.6-1 1.1-1.3 1.4l-.3.3c-4.6 4.6-5.9 11.4-3.4 17.4 2.5 6 8.3 9.9 14.8 9.9 28.7 0 57.6-8.9 81.6-19.3 22.9-10 42.4-21.9 54.3-30.6 31.8 11.5 67 17.9 104.1 17.9zM128 208a32 32 0 1 1 0 64 32 32 0 1 1 0-64zm128 0a32 32 0 1 1 0 64 32 32 0 1 1 0-64zm96 32a32 32 0 1 1 64 0 32 32 0 1 1-64 0z"/></svg> </a> <a href=mailto:cms-conveners-ml-knowledge@cern.ch target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l217.6 163.2c11.4 8.5 27 8.5 38.4 0l217.6-163.2c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48H48zM0 176v208c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V176L294.4 339.2a63.9 63.9 0 0 1-76.8 0L0 176z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["instant", "navigation.sections"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../assets/javascripts/bundle.ebd0bdb7.min.js></script> <script src=https://unpkg.com/mermaid@10.9/dist/mermaid.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../termynal.js></script> </body> </html>