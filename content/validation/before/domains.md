Data plays a crucial role in the process of training any ML model. It is something from which the model learns to solve a given task and therefore care needs to be taken with its handling:

1. By making sure that a data set is relevant to the problem and no deviation from expectations is introduced. This makes sure that the model doesn't incorporate undesired fluctuations or features.
1. By performing a proper preprocessing of the data set so that the training step goes smoothly.

In this section a general [domain perspective](https://en.wikipedia.org/wiki/Domain_adaptation) on data will be covered. In the following sections a more granular look will be taken from the side of features and construction of inputs to the model.

## Coverage  
To begin with, one needs to bear in mind that training data should be as close as possible to data they expect to have in the context of analysis. Speaking in more formal terms,
> Domains of training (used to train the model) and inference (used to make final predictions) data sets should not sizeably diverge.

??? example "Examples"
    * In most of the cases the model is usually trained on MC simulated data and later on applied to data to produce predictions which are then passed on to statistical inference step. MC simulation isn't perfect and therefore there are always differences between simulation and data domains. This can lead to the cases when model learns simulation artefacts which come e.g. from detector response mismodelling. Thus, its performance on data may be at least suboptimal and at most meaningless.
    * Consider the model which is trained to predict the energy of a hadron given its energy deposits in the calorimeter (represented e.g. in the form of image or graph). Data consists of the showers initiated by a particle generated by a particle gun and having discrete values of energies (e.g. 1 GeV, 10 GeV, 20 GeV, etc.). However, in the real world settings, the model will be applied to showers produced by particles with underlying continuous energy spectrum. Although ML models are known for their capability to interpolate beyond their training domain, without apropriate tests model performance in the parts of the energy spectrum outside of its training domain is not a priori clear.  

### Solution    

It is a widely known practice to introduce **scale factor** (SF) **corrections** to account for possible discrepancies between data and MC simulation. Effectively, that means that the model is probed on some part of the domain on which it wasn't trained on (data) and then corrected for any differences by using a meaningful set of observables to derive SFs. One particularly promising approaches to remedy for data/MC domain difference is to use adversarial approaches to fully leverage the multidimensionality of the problem, as described in a [DeepSF note](https://cds.cern.ch/record/2666647).

Another solution would be to incorporate methods of [**domain adaptation**](https://github.com/zhaoxin94/awesome-domain-adaptation) into an ML pipeline, which essentially guide the model to be invariant and robust towards domain shift. Particularly in HEP, a [Learning to Pivot with Adversarial Networks](https://arxiv.org/pdf/1611.01046.pdf) paper was one of the pioneers to investigate how a pile-up dependency can be mitigated, which can also be easily expanded to building a model robust to domain shift[^1].  

Last but not the least, a usage of [**bayesian neural networks**]() has a great advantage of getting uncertainties estimate along with each prediction. If these uncertainties are significantly larger for some samples, this could indicate that they come from the domain beyond the training one (a so-called [out-of-distribution samples](https://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html)) - and therefore care needs to be taken in that cases.

## Population
Furthermore, nowadays analyses are searching for very rare processes and therefore are interested in low-populated regions of the phase space. And even though the domain of interest can be covered in the training data set, it may also happen that not sufficiently in terms of data points populating those regions. That makes the model behaviour on an event which falls into those regions unpredictable - because it couldn't learn how to generalise in those areas due to a lack of data to learn from. Therefore,
> It is important to make sure that the phase space of interest is well-represented in the training data set.

??? example "Example"
    This is what is often called in HEP jargon "little statistics in the tails": meaning that too few events can be found in the tails of the corresponding distribution, e.g. in the high-pt region. This might be important because the topology of events changes when one enters high-pt areas of the phase space (aka boosted regime). This further means that the model should be able to capture this change in the event signature. However, it might fail to do so due to a little available data to learn from comparing to a low-pt region.   

### Solution    

Clearly, a way out in that case would be to **provide enough training data** to cover those regions (also ensuring that the model has enough [capacity](model.md) to embrace diverse and complex topologies). 

Another solution would be to communicate to the model importance of specific topologies, which can be done for example by **upweighting** those events' contribution to the [loss function](metrics.md). 

Lastly, it might be worth trying to **train several models**, each targeting its specific region, instead of a general-purpose one (e.g. low-pt & boosted/merged topology tagger). Effectively, factorisation of various regions disentangle the problem of their separation for a single model and delegates it to an ensemble of dedicated models, each targeting its specific region.

[^1]: From that paper on, the HEP community started to explore a similar topic of _model decorrelation_, i.e. how to build a model which would be invariant to a particular variable or property of data. For a more detailed overview please refer to Section 2 of [this paper](https://arxiv.org/abs/2010.09745).  
