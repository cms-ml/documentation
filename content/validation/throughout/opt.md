<figure>
<img src="../../../images/validation/loss_surface.png"/>
<figcaption>Figure 1.  The loss surfaces of ResNet-56 with/without skip connections. [source: "Visualizing the Loss Landscape of Neural Nets" paper]</figcaption>
</figure>

However, it might be that for a given task overfitting is of no concern, but there are still instabilities in loss function convergence happening during the training[^1]. The loss landscape is a [complex object](https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets) having multiple local minima and which is moreover not at all understood due to the high dimensionality of the problem. That makes the gradient descent procedure of finding a minimum not that simple. However, if instabilities are observed, there are a few common things which could explain that: 

* The main candidate for a problem might be the [**learning rate**](https://en.wikipedia.org/wiki/Learning_rate) (LR). Being an important hyperparameter which steers the optimisation, setting it too high make cause extremily stochastic behaviour which will likely cause the optimisation to get stuck in some random minimum being way far from optimum. Oppositely, setting it too low may cause the convergence to take very long time. The optimal value in between those extremes can still be problematic due to a chance of getting stuck in a local minimum on the way towards a better one. That is why several approaches on [LR schedulers](https://pytorch.org/docs/stable/optim.html) (e.g. [cosine annealing](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR)) and also [adaptive LR](https://cs231n.github.io/neural-networks-3/#ada) (e.g. [Adam](https://arxiv.org/abs/1412.6980) being the most prominent one) have been developed to have more flexibility during the training, as opposed to setting LR fixed from the very beginning of the training until its end.

* Another possibility is that there are **NaN/inf values** or uniformities/outliers appearing in the input batches. It can cause the gradient updates to go beyond the normal scale and therefore dramatically affect the stability of the loss optimisation. This can be avoided by careful [data preprocessing](../before/inputs.md) and batch formation.  

* Last but not the least, there is a chance that **gradients will explode or vanish** during the training, which will reveal itself as a rapid increase/stagnation in the loss function values. This is largely the feature of deep architectures, where during the [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) gradients are accumulated from one layer to another, and therefore any minor deviations in scale can exponentially amplify/diminish as they get multiplied. Since it is the scale of the trainable weights themselves which defines the weight gradients, a proper [weight initialisation](https://cs231n.github.io/neural-networks-2/#init) can foster smooth and consistent gradient updates. Also, [batch normalisation together with weight standartization](https://arxiv.org/abs/1903.10520) showed to be a powerful technique to consistently improve performance across various domains. Finally, a choice of [activation function](https://en.wikipedia.org/wiki/Activation_function) is particularly important since it directly contributes to a gradient computation. For example, a [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) is known to cause gradients to vanish due to its gradient being 0 at large input values. Therefore, it is often suggested to stick to classical [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) or try other [alternatives](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions) to see if it brings improvement in performance. 


[^1]: Sometimes particularly [peculiar](https://lossfunctions.tumblr.com).